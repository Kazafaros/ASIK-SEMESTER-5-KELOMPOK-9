import json

# Create prediction model notebook
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fase 7: HSI Prediction Model\n",
                "\n",
                "Notebook ini untuk membuat model prediksi HSI menggunakan ARIMA.\n",
                "\n",
                "## Langkah-langkah:\n",
                "1. Load data historis (monthly HSI 2021-2024)\n",
                "2. Preprocess data (interpolate missing values)\n",
                "3. Train ARIMA model per grid point\n",
                "4. Predict HSI untuk tahun 2025-2030 (per tahun)\n",
                "5. Generate GeoJSON untuk prediksi\n",
                "6. Save prediction results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "from datetime import datetime\n",
                "from scipy.interpolate import interp1d\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ARIMA libraries\n",
                "try:\n",
                "    from pmdarima import auto_arima\n",
                "    print(\"✅ pmdarima imported successfully\")\n",
                "except ImportError:\n",
                "    print(\"⚠️  pmdarima not found. Installing...\")\n",
                "    import subprocess\n",
                "    subprocess.check_call(['pip', 'install', 'pmdarima'])\n",
                "    from pmdarima import auto_arima\n",
                "    print(\"✅ pmdarima installed and imported\")\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load monthly HSI data\n",
                "MONTHLY_DATA_FILE = '../data/processed/monthly_hsi_data.npz'\n",
                "\n",
                "if not os.path.exists(MONTHLY_DATA_FILE):\n",
                "    raise FileNotFoundError(f\"Monthly data file not found! Please run monthly aggregation notebook first.\")\n",
                "\n",
                "data = np.load(MONTHLY_DATA_FILE)\n",
                "\n",
                "monthly_hsi = data['hsi_total']  # Shape: [48 months, lat, lon]\n",
                "lat_grid = data['lat_grid']\n",
                "lon_grid = data['lon_grid']\n",
                "months = data['months']\n",
                "\n",
                "if isinstance(months, np.ndarray):\n",
                "    months = months.tolist()\n",
                "\n",
                "print(f\"✅ Monthly data loaded successfully!\")\n",
                "print(f\"\\nData shapes:\")\n",
                "print(f\"  HSI: {monthly_hsi.shape}\")\n",
                "print(f\"  Grid: {len(lat_grid)} x {len(lon_grid)} = {len(lat_grid) * len(lon_grid)} points\")\n",
                "print(f\"  Months: {len(months)}\")\n",
                "print(f\"\\nDate range: {months[0]} to {months[-1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocess Data (Interpolate Missing Values)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def interpolate_missing_values(time_series):\n",
                "    \"\"\"\n",
                "    Interpolate missing values in time series\n",
                "    \"\"\"\n",
                "    if len(time_series) == 0:\n",
                "        return time_series\n",
                "    \n",
                "    # Convert to pandas Series for easier handling\n",
                "    ts_series = pd.Series(time_series)\n",
                "    \n",
                "    # Interpolate missing values\n",
                "    ts_interpolated = ts_series.interpolate(method='linear', limit_direction='both')\n",
                "    \n",
                "    # If still have NaN at the beginning/end, forward/backward fill\n",
                "    ts_interpolated = ts_interpolated.fillna(method='ffill').fillna(method='bfill')\n",
                "    \n",
                "    # If still NaN, fill with mean\n",
                "    if ts_interpolated.isna().any():\n",
                "        ts_interpolated = ts_interpolated.fillna(ts_interpolated.mean())\n",
                "    \n",
                "    return ts_interpolated.values\n",
                "\n",
                "print(\"✅ Interpolation function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess all grid points\n",
                "print(\"Preprocessing data (interpolating missing values)...\")\n",
                "\n",
                "n_lat, n_lon = len(lat_grid), len(lon_grid)\n",
                "n_months = len(months)\n",
                "\n",
                "hsi_processed = np.full((n_months, n_lat, n_lon), np.nan)\n",
                "\n",
                "for i in range(n_lat):\n",
                "    for j in range(n_lon):\n",
                "        time_series = monthly_hsi[:, i, j]\n",
                "        hsi_processed[:, i, j] = interpolate_missing_values(time_series)\n",
                "    \n",
                "    if (i + 1) % 5 == 0:\n",
                "        print(f\"  Processed {i+1}/{n_lat} rows...\")\n",
                "\n",
                "print(f\"✅ Data preprocessing complete!\")\n",
                "print(f\"Missing values before: {np.sum(np.isnan(monthly_hsi)):,}\")\n",
                "print(f\"Missing values after: {np.sum(np.isnan(hsi_processed)):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train ARIMA Model per Grid Point"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from multiprocessing import Pool, cpu_count\n",
                "from functools import partial\n",
                "\n",
                "def train_arima_for_point(args):\n",
                "    \"\"\"\n",
                "    Train ARIMA model for a single grid point\n",
                "    \"\"\"\n",
                "    i, j, time_series = args\n",
                "    \n",
                "    try:\n",
                "        # Skip if all NaN or constant values\n",
                "        if np.all(np.isnan(time_series)) or np.nanstd(time_series) == 0:\n",
                "            return (i, j, None, None)\n",
                "        \n",
                "        # Auto ARIMA - find best parameters\n",
                "        model = auto_arima(\n",
                "            time_series,\n",
                "            start_p=0, start_q=0,\n",
                "            max_p=5, max_q=5,\n",
                "            seasonal=False,  # No seasonality for now (can enable if needed)\n",
                "            stepwise=True,\n",
                "            suppress_warnings=True,\n",
                "            error_action='ignore',\n",
                "            max_order=10,\n",
                "            n_jobs=1\n",
                "        )\n",
                "        \n",
                "        return (i, j, model, model.order)\n",
                "    except Exception as e:\n",
                "        print(f\"Error training model for point ({i}, {j}): {e}\")\n",
                "        return (i, j, None, None)\n",
                "\n",
                "print(\"✅ ARIMA training function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for training\n",
                "print(\"=== Training ARIMA Models ===\")\n",
                "print(f\"Training {n_lat * n_lon} models (one per grid point)...\")\n",
                "\n",
                "training_data = []\n",
                "for i in range(n_lat):\n",
                "    for j in range(n_lon):\n",
                "        time_series = hsi_processed[:, i, j]\n",
                "        training_data.append((i, j, time_series))\n",
                "\n",
                "print(f\"Total grid points: {len(training_data)}\")\n",
                "\n",
                "# Train models (with parallel processing)\n",
                "USE_PARALLEL = True\n",
                "N_WORKERS = min(4, cpu_count())\n",
                "\n",
                "start_time = time.time()\n",
                "trained_models = {}\n",
                "\n",
                "if USE_PARALLEL and N_WORKERS > 1:\n",
                "    print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
                "    with Pool(processes=N_WORKERS) as pool:\n",
                "        results = pool.map(train_arima_for_point, training_data)\n",
                "    \n",
                "    for i, j, model, order in results:\n",
                "        if model is not None:\n",
                "            trained_models[(i, j)] = model\n",
                "else:\n",
                "    print(\"Using sequential processing...\")\n",
                "    for idx, args in enumerate(training_data):\n",
                "        i, j, model, order = train_arima_for_point(args)\n",
                "        if model is not None:\n",
                "            trained_models[(i, j)] = model\n",
                "        \n",
                "        if (idx + 1) % 50 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
                "            remaining = (len(training_data) - idx - 1) / rate if rate > 0 else 0\n",
                "            print(f\"  Progress: {idx+1}/{len(training_data)} ({100*(idx+1)/len(training_data):.1f}%) | \"\n",
                "                  f\"Elapsed: {elapsed:.1f}s | ETA: {remaining:.1f}s\")\n",
                "\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"\\n✅ Model training complete in {elapsed:.1f}s ({elapsed/60:.1f} minutes)!\")\n",
                "print(f\"Successfully trained: {len(trained_models)}/{len(training_data)} models\")\n",
                "print(f\"Failed: {len(training_data) - len(trained_models)} models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Predict HSI for Future Years (2025-2030)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict for years 2025-2030\n",
                "PREDICTION_YEARS = [2025, 2026, 2027, 2028, 2029, 2030]\n",
                "MONTHS_PER_YEAR = 12\n",
                "\n",
                "print(f\"=== Predicting HSI for {PREDICTION_YEARS} ===\")\n",
                "\n",
                "# Store predictions: {year: [n_lat, n_lon] array}\n",
                "predictions = {}\n",
                "\n",
                "for year in PREDICTION_YEARS:\n",
                "    print(f\"\\nPredicting for year {year}...\")\n",
                "    \n",
                "    # Calculate months to predict\n",
                "    # Last month in data is 2024-12 (index 47)\n",
                "    # For 2025: predict months 48-59 (12 months)\n",
                "    year_index = year - 2025\n",
                "    start_month_idx = 48 + (year_index * 12)\n",
                "    n_steps = 12  # Predict 12 months per year\n",
                "    \n",
                "    prediction_array = np.full((n_lat, n_lon), np.nan)\n",
                "    \n",
                "    for (i, j), model in trained_models.items():\n",
                "        try:\n",
                "            # Predict 12 months ahead\n",
                "            forecast = model.predict(n_periods=n_steps)\n",
                "            \n",
                "            # Average to get annual value\n",
                "            annual_value = np.mean(forecast)\n",
                "            \n",
                "            # Clip to valid range [0, 1]\n",
                "            annual_value = np.clip(annual_value, 0.0, 1.0)\n",
                "            \n",
                "            prediction_array[i, j] = annual_value\n",
                "        except Exception as e:\n",
                "            print(f\"Error predicting for point ({i}, {j}): {e}\")\n",
                "            continue\n",
                "    \n",
                "    predictions[year] = prediction_array\n",
                "    print(f\"  ✅ Year {year}: {np.sum(~np.isnan(prediction_array))} valid predictions\")\n",
                "\n",
                "print(f\"\\n✅ Prediction complete for {len(PREDICTION_YEARS)} years!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Generate GeoJSON for Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_prediction_geojson(year, prediction_array, lat_grid, lon_grid):\n",
                "    \"\"\"\n",
                "    Create GeoJSON from prediction array\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    \n",
                "    # Create meshgrid\n",
                "    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
                "    \n",
                "    for i in range(len(lat_grid)):\n",
                "        for j in range(len(lon_grid)):\n",
                "            lat = lat_mesh[i, j]\n",
                "            lon = lon_mesh[i, j]\n",
                "            hsi_val = float(prediction_array[i, j]) if not np.isnan(prediction_array[i, j]) else None\n",
                "            \n",
                "            # Skip if NaN\n",
                "            if hsi_val is None:\n",
                "                continue\n",
                "            \n",
                "            feature = {\n",
                "                \"type\": \"Feature\",\n",
                "                \"geometry\": {\n",
                "                    \"type\": \"Point\",\n",
                "                    \"coordinates\": [float(lon), float(lat)]\n",
                "                },\n",
                "                \"properties\": {\n",
                "                    \"hsi\": hsi_val,\n",
                "                    \"year\": year,\n",
                "                    \"is_prediction\": True\n",
                "                }\n",
                "            }\n",
                "            \n",
                "            features.append(feature)\n",
                "    \n",
                "    geojson = {\n",
                "        \"type\": \"FeatureCollection\",\n",
                "        \"features\": features\n",
                "    }\n",
                "    \n",
                "    return geojson\n",
                "\n",
                "print(\"✅ GeoJSON creation function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate GeoJSON files for each prediction year\n",
                "OUTPUT_DIR = '../data/predictions'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"=== Generating GeoJSON Files ===\")\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")\n",
                "\n",
                "prediction_files = []\n",
                "\n",
                "for year in PREDICTION_YEARS:\n",
                "    prediction_array = predictions[year]\n",
                "    geojson = create_prediction_geojson(year, prediction_array, lat_grid, lon_grid)\n",
                "    \n",
                "    # Save GeoJSON\n",
                "    filename = f\"prediction_{year}.geojson\"\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    \n",
                "    with open(filepath, 'w', encoding='utf-8') as f:\n",
                "        json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    prediction_files.append({\n",
                "        'year': year,\n",
                "        'file': filename,\n",
                "        'features': len(geojson['features'])\n",
                "    })\n",
                "    \n",
                "    print(f\"  ✅ Saved {filename} ({len(geojson['features'])} features)\")\n",
                "\n",
                "print(f\"\\n✅ Generated {len(prediction_files)} prediction files!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generate Metadata for Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create metadata for predictions\n",
                "metadata = {\n",
                "    \"title\": \"HSI Predictions for Sunda Strait\",\n",
                "    \"description\": \"Predicted Habitat Suitability Index values for future years using ARIMA model\",\n",
                "    \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "    \"model_type\": \"ARIMA\",\n",
                "    \"training_data\": {\n",
                "        \"start\": months[0],\n",
                "        \"end\": months[-1],\n",
                "        \"total_months\": len(months)\n",
                "    },\n",
                "    \"prediction_years\": PREDICTION_YEARS,\n",
                "    \"spatial_bounds\": {\n",
                "        \"min_lat\": float(lat_grid.min()),\n",
                "        \"max_lat\": float(lat_grid.max()),\n",
                "        \"min_lon\": float(lon_grid.min()),\n",
                "        \"max_lon\": float(lon_grid.max())\n",
                "    },\n",
                "    \"available_predictions\": prediction_files,\n",
                "    \"total_predictions\": len(prediction_files),\n",
                "    \"models_trained\": len(trained_models),\n",
                "    \"note\": \"Predictions are based on historical data (2021-2024) and should be used as estimates only.\"\n",
                "}\n",
                "\n",
                "# Save metadata\n",
                "metadata_file = os.path.join(OUTPUT_DIR, 'metadata.json')\n",
                "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
                "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"✅ Metadata saved to {metadata_file}\")\n",
                "print(f\"\\nMetadata summary:\")\n",
                "print(f\"  Training data: {metadata['training_data']['start']} to {metadata['training_data']['end']}\")\n",
                "print(f\"  Prediction years: {metadata['prediction_years']}\")\n",
                "print(f\"  Models trained: {metadata['models_trained']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Trained Models (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model parameters (not the full model objects, just parameters)\n",
                "# This allows us to recreate models later if needed\n",
                "MODEL_DIR = '../data/models'\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "model_params = {}\n",
                "for (i, j), model in trained_models.items():\n",
                "    model_params[f\"{i}_{j}\"] = {\n",
                "        'order': model.order,\n",
                "        'seasonal_order': model.seasonal_order if hasattr(model, 'seasonal_order') else None\n",
                "    }\n",
                "\n",
                "# Save as JSON (only parameters, not full model)\n",
                "params_file = os.path.join(MODEL_DIR, 'arima_parameters.json')\n",
                "with open(params_file, 'w', encoding='utf-8') as f:\n",
                "    json.dump(model_params, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"✅ Model parameters saved to {params_file}\")\n",
                "print(f\"  Total models: {len(model_params)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary & Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== PREDICTION MODEL SUMMARY ===\")\n",
                "print(\"\\n✅ Prediction model training and prediction completed!\")\n",
                "print(\"\\nWhat was done:\")\n",
                "print(\"1. ✅ Loaded monthly HSI data (2021-2024)\")\n",
                "print(\"2. ✅ Preprocessed data (interpolated missing values)\")\n",
                "print(\"3. ✅ Trained ARIMA models per grid point\")\n",
                "print(f\"4. ✅ Predicted HSI for {len(PREDICTION_YEARS)} years ({PREDICTION_YEARS[0]}-{PREDICTION_YEARS[-1]})\")\n",
                "print(\"5. ✅ Generated GeoJSON files for predictions\")\n",
                "print(\"6. ✅ Created metadata\")\n",
                "print(\"\\nOutput files:\")\n",
                "print(f\"  - {len(prediction_files)} prediction GeoJSON files in data/predictions/\")\n",
                "print(f\"  - metadata.json with prediction information\")\n",
                "print(f\"  - Model parameters saved\")\n",
                "print(\"\\nNext Steps:\")\n",
                "print(\"- Backend: Create API endpoints for predictions\")\n",
                "print(\"  - GET /api/predict?year=2025\")\n",
                "print(\"  - GET /api/predict?year=2025&sst_delta=1.0&chl_delta=0.2&salinity_delta=0.5\")\n",
                "print(\"- Frontend: Integrate prediction visualization\")\n",
                "print(\"  - Load prediction GeoJSON\")\n",
                "print(\"  - Display as heatmap\")\n",
                "print(\"  - Add custom scenario controls\")\n",
                "print(\"\\n✅ Jupyter prediction phase complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write notebook to file
with open('07_prediction_model.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print("Prediction notebook created successfully!")

