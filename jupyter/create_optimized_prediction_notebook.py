import json

# Create optimized prediction model notebook with reduced grid resolution
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fase 7: HSI Prediction Model (Optimized)\n",
                "\n",
                "Notebook ini untuk membuat model prediksi HSI menggunakan ARIMA dengan optimasi.\n",
                "\n",
                "## Optimasi:\n",
                "- Reduced grid resolution (0.1° instead of 0.05°)\n",
                "- Walk-forward validation\n",
                "- All metrics (R², MAE, RMSE, MAPE)\n",
                "- Visualization: actual vs predicted\n",
                "\n",
                "## Langkah-langkah:\n",
                "1. Load data historis (monthly HSI 2021-2024)\n",
                "2. Reduce grid resolution untuk training yang lebih cepat\n",
                "3. Preprocess data (interpolate missing values)\n",
                "4. Walk-forward validation\n",
                "5. Train ARIMA model per grid point\n",
                "6. Predict HSI untuk tahun 2025-2030 (per tahun)\n",
                "7. Generate GeoJSON untuk prediksi\n",
                "8. Calculate metrics dan visualisasi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "from datetime import datetime\n",
                "from scipy.interpolate import griddata\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ARIMA libraries\n",
                "try:\n",
                "    from pmdarima import auto_arima\n",
                "    print(\"✅ pmdarima imported successfully\")\n",
                "except ImportError:\n",
                "    print(\"⚠️  pmdarima not found. Installing...\")\n",
                "    import subprocess\n",
                "    subprocess.check_call(['pip', 'install', 'pmdarima'])\n",
                "    from pmdarima import auto_arima\n",
                "    print(\"✅ pmdarima installed and imported\")\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load monthly HSI data\n",
                "MONTHLY_DATA_FILE = '../data/processed/monthly_hsi_data.npz'\n",
                "\n",
                "if not os.path.exists(MONTHLY_DATA_FILE):\n",
                "    raise FileNotFoundError(f\"Monthly data file not found! Please run monthly aggregation notebook first.\")\n",
                "\n",
                "data = np.load(MONTHLY_DATA_FILE)\n",
                "\n",
                "monthly_hsi = data['hsi_total']  # Shape: [48 months, lat, lon]\n",
                "lat_grid_orig = data['lat_grid']\n",
                "lon_grid_orig = data['lon_grid']\n",
                "months = data['months']\n",
                "\n",
                "if isinstance(months, np.ndarray):\n",
                "    months = months.tolist()\n",
                "\n",
                "print(f\"✅ Monthly data loaded successfully!\")\n",
                "print(f\"\\nOriginal data shapes:\")\n",
                "print(f\"  HSI: {monthly_hsi.shape}\")\n",
                "print(f\"  Grid: {len(lat_grid_orig)} x {len(lon_grid_orig)} = {len(lat_grid_orig) * len(lon_grid_orig)} points\")\n",
                "print(f\"  Months: {len(months)}\")\n",
                "print(f\"\\nDate range: {months[0]} to {months[-1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Reduce Grid Resolution (Optimization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reduce grid resolution untuk training yang lebih cepat\n",
                "# Dari 0.05° ke 0.1° (reduce dari ~812 points ke ~200 points)\n",
                "TARGET_RESOLUTION = 0.1  # degrees\n",
                "\n",
                "# Create new grid dengan resolution lebih besar\n",
                "lat_min, lat_max = lat_grid_orig.min(), lat_grid_orig.max()\n",
                "lon_min, lon_max = lon_grid_orig.min(), lon_grid_orig.max()\n",
                "\n",
                "lat_grid = np.arange(lat_min, lat_max + TARGET_RESOLUTION, TARGET_RESOLUTION)\n",
                "lon_grid = np.arange(lon_min, lon_max + TARGET_RESOLUTION, TARGET_RESOLUTION)\n",
                "\n",
                "print(f\"=== Grid Resolution Reduction ===\")\n",
                "print(f\"Original grid: {len(lat_grid_orig)} x {len(lon_grid_orig)} = {len(lat_grid_orig) * len(lon_grid_orig)} points\")\n",
                "print(f\"Reduced grid: {len(lat_grid)} x {len(lon_grid)} = {len(lat_grid) * len(lon_grid)} points\")\n",
                "print(f\"Reduction: {100 * (1 - (len(lat_grid) * len(lon_grid)) / (len(lat_grid_orig) * len(lon_grid_orig))):.1f}% fewer points\")\n",
                "\n",
                "# Resample data ke grid baru\n",
                "print(f\"\\nResampling data to new grid...\")\n",
                "lon_mesh_orig, lat_mesh_orig = np.meshgrid(lon_grid_orig, lat_grid_orig)\n",
                "lon_mesh_new, lat_mesh_new = np.meshgrid(lon_grid, lat_grid)\n",
                "\n",
                "# Resample untuk setiap bulan\n",
                "monthly_hsi_resampled = np.full((len(months), len(lat_grid), len(lon_grid)), np.nan)\n",
                "\n",
                "for t in range(len(months)):\n",
                "    points_orig = np.column_stack((lon_mesh_orig.ravel(), lat_mesh_orig.ravel()))\n",
                "    values_orig = monthly_hsi[t, :, :].ravel()\n",
                "    points_new = np.column_stack((lon_mesh_new.ravel(), lat_mesh_new.ravel()))\n",
                "    \n",
                "    # Interpolate\n",
                "    from scipy.interpolate import griddata\n",
                "    values_new = griddata(\n",
                "        points_orig,\n",
                "        values_orig,\n",
                "        points_new,\n",
                "        method='linear',\n",
                "        fill_value=np.nan\n",
                "    )\n",
                "    \n",
                "    monthly_hsi_resampled[t, :, :] = values_new.reshape(lon_mesh_new.shape)\n",
                "    \n",
                "    if (t + 1) % 12 == 0:\n",
                "        print(f\"  Resampled {t+1}/{len(months)} months...\")\n",
                "\n",
                "print(f\"✅ Grid resampling complete!\")\n",
                "print(f\"New data shape: {monthly_hsi_resampled.shape}\")\n",
                "\n",
                "# Update variable\n",
                "monthly_hsi = monthly_hsi_resampled\n",
                "\n",
                "# Define grid dimensions (needed for later cells)\n",
                "n_lat, n_lon = len(lat_grid), len(lon_grid)\n",
                "n_months = len(months)\n",
                "print(f\"\\nGrid dimensions: {n_lat} x {n_lon} = {n_lat * n_lon} points\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocess Data (Interpolate Missing Values)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def interpolate_missing_values(time_series):\n",
                "    \"\"\"\n",
                "    Interpolate missing values in time series\n",
                "    \"\"\"\n",
                "    if len(time_series) == 0:\n",
                "        return time_series\n",
                "    \n",
                "    ts_series = pd.Series(time_series)\n",
                "    ts_interpolated = ts_series.interpolate(method='linear', limit_direction='both')\n",
                "    ts_interpolated = ts_interpolated.fillna(method='ffill').fillna(method='bfill')\n",
                "    \n",
                "    if ts_interpolated.isna().any():\n",
                "        ts_interpolated = ts_interpolated.fillna(ts_interpolated.mean())\n",
                "    \n",
                "    return ts_interpolated.values\n",
                "\n",
                "print(\"✅ Interpolation function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess all grid points\n",
                "print(\"Preprocessing data (interpolating missing values)...\")\n",
                "\n",
                "n_lat, n_lon = len(lat_grid), len(lon_grid)\n",
                "n_months = len(months)\n",
                "\n",
                "hsi_processed = np.full((n_months, n_lat, n_lon), np.nan)\n",
                "\n",
                "for i in range(n_lat):\n",
                "    for j in range(n_lon):\n",
                "        time_series = monthly_hsi[:, i, j]\n",
                "        hsi_processed[:, i, j] = interpolate_missing_values(time_series)\n",
                "\n",
                "print(f\"✅ Data preprocessing complete!\")\n",
                "print(f\"Missing values before: {np.sum(np.isnan(monthly_hsi)):,}\")\n",
                "print(f\"Missing values after: {np.sum(np.isnan(hsi_processed)):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Walk-Forward Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Walk-forward validation\n",
                "# Train dengan data awal, test dengan data berikutnya, expand training set\n",
                "\n",
                "print(\"=== Walk-Forward Validation ===\")\n",
                "\n",
                "# Split: Train dengan 36 bulan pertama (2021-2023), test dengan 12 bulan terakhir (2024)\n",
                "TRAIN_MONTHS = 36  # 2021-2023\n",
                "TEST_MONTHS = 12   # 2024\n",
                "\n",
                "# Prepare validation data\n",
                "validation_results = []\n",
                "\n",
                "# Sample beberapa grid points untuk validasi (untuk speed)\n",
                "VALIDATION_SAMPLE = min(50, n_lat * n_lon)  # Sample 50 points untuk validasi\n",
                "np.random.seed(42)\n",
                "validation_indices = np.random.choice(n_lat * n_lon, VALIDATION_SAMPLE, replace=False)\n",
                "\n",
                "print(f\"Validating {VALIDATION_SAMPLE} sample grid points...\")\n",
                "print(f\"Training period: months 0-{TRAIN_MONTHS-1} ({months[0]} to {months[TRAIN_MONTHS-1]})\")\n",
                "print(f\"Testing period: months {TRAIN_MONTHS}-{TRAIN_MONTHS+TEST_MONTHS-1} ({months[TRAIN_MONTHS]} to {months[TRAIN_MONTHS+TEST_MONTHS-1]})\")\n",
                "\n",
                "for idx, point_idx in enumerate(validation_indices):\n",
                "    i = point_idx // n_lon\n",
                "    j = point_idx % n_lon\n",
                "    \n",
                "    time_series = hsi_processed[:, i, j]\n",
                "    train_data = time_series[:TRAIN_MONTHS]\n",
                "    test_data = time_series[TRAIN_MONTHS:TRAIN_MONTHS+TEST_MONTHS]\n",
                "    \n",
                "    # Skip if invalid\n",
                "    if np.all(np.isnan(train_data)) or np.nanstd(train_data) == 0:\n",
                "        continue\n",
                "    \n",
                "    try:\n",
                "        # Train model\n",
                "        model = auto_arima(\n",
                "            train_data,\n",
                "            start_p=0, start_q=0,\n",
                "            max_p=3, max_q=3,  # Reduced untuk speed\n",
                "            seasonal=False,\n",
                "            stepwise=True,\n",
                "            suppress_warnings=True,\n",
                "            error_action='ignore',\n",
                "            max_order=6,  # Reduced\n",
                "            n_jobs=1\n",
                "        )\n",
                "        \n",
                "        # Predict test period\n",
                "        forecast = model.predict(n_periods=TEST_MONTHS)\n",
                "        \n",
                "        # Calculate metrics\n",
                "        r2 = r2_score(test_data, forecast)\n",
                "        mae = mean_absolute_error(test_data, forecast)\n",
                "        rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
                "        mape = np.mean(np.abs((test_data - forecast) / (test_data + 1e-10))) * 100\n",
                "        \n",
                "        validation_results.append({\n",
                "            'point': (i, j),\n",
                "            'r2': r2,\n",
                "            'mae': mae,\n",
                "            'rmse': rmse,\n",
                "            'mape': mape,\n",
                "            'actual': test_data.tolist(),\n",
                "            'predicted': forecast.tolist()\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        continue\n",
                "    \n",
                "    if (idx + 1) % 10 == 0:\n",
                "        print(f\"  Validated {idx+1}/{VALIDATION_SAMPLE} points...\")\n",
                "\n",
                "print(f\"\\n✅ Validation complete!\")\n",
                "print(f\"Successfully validated: {len(validation_results)}/{VALIDATION_SAMPLE} points\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Validation Metrics Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate summary metrics\n",
                "if validation_results:\n",
                "    r2_scores = [r['r2'] for r in validation_results]\n",
                "    mae_scores = [r['mae'] for r in validation_results]\n",
                "    rmse_scores = [r['rmse'] for r in validation_results]\n",
                "    mape_scores = [r['mape'] for r in validation_results]\n",
                "    \n",
                "    print(\"=== VALIDATION METRICS SUMMARY ===\")\n",
                "    print(f\"\\nR² Score:\")\n",
                "    print(f\"  Mean: {np.mean(r2_scores):.4f}\")\n",
                "    print(f\"  Median: {np.median(r2_scores):.4f}\")\n",
                "    print(f\"  Min: {np.min(r2_scores):.4f}\")\n",
                "    print(f\"  Max: {np.max(r2_scores):.4f}\")\n",
                "    \n",
                "    print(f\"\\nMAE (Mean Absolute Error):\")\n",
                "    print(f\"  Mean: {np.mean(mae_scores):.4f}\")\n",
                "    print(f\"  Median: {np.median(mae_scores):.4f}\")\n",
                "    \n",
                "    print(f\"\\nRMSE (Root Mean Squared Error):\")\n",
                "    print(f\"  Mean: {np.mean(rmse_scores):.4f}\")\n",
                "    print(f\"  Median: {np.median(rmse_scores):.4f}\")\n",
                "    \n",
                "    print(f\"\\nMAPE (Mean Absolute Percentage Error):\")\n",
                "    print(f\"  Mean: {np.mean(mape_scores):.2f}%\")\n",
                "    print(f\"  Median: {np.median(mape_scores):.2f}%\")\n",
                "    \n",
                "    # Interpretation\n",
                "    mean_r2 = np.mean(r2_scores)\n",
                "    mean_mae = np.mean(mae_scores)\n",
                "    \n",
                "    print(f\"\\n=== MODEL PERFORMANCE INTERPRETATION ===\")\n",
                "    if mean_r2 > 0.7:\n",
                "        print(f\"✅ R² = {mean_r2:.3f}: Model performance EXCELLENT\")\n",
                "    elif mean_r2 > 0.5:\n",
                "        print(f\"⚠️  R² = {mean_r2:.3f}: Model performance GOOD\")\n",
                "    elif mean_r2 > 0.3:\n",
                "        print(f\"⚠️  R² = {mean_r2:.3f}: Model performance MODERATE\")\n",
                "    else:\n",
                "        print(f\"❌ R² = {mean_r2:.3f}: Model performance POOR - needs improvement\")\n",
                "    \n",
                "    if mean_mae < 0.1:\n",
                "        print(f\"✅ MAE = {mean_mae:.4f}: Average error is LOW (< 10% of range)\")\n",
                "    elif mean_mae < 0.2:\n",
                "        print(f\"⚠️  MAE = {mean_mae:.4f}: Average error is MODERATE (10-20% of range)\")\n",
                "    else:\n",
                "        print(f\"❌ MAE = {mean_mae:.4f}: Average error is HIGH (> 20% of range)\")\n",
                "else:\n",
                "    print(\"⚠️  No validation results available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization: Actual vs Predicted"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot actual vs predicted untuk beberapa sample points\n",
                "if validation_results:\n",
                "    # Select best, worst, and average performing points\n",
                "    r2_scores = [r['r2'] for r in validation_results]\n",
                "    best_idx = np.argmax(r2_scores)\n",
                "    worst_idx = np.argmin(r2_scores)\n",
                "    median_idx = np.argsort(r2_scores)[len(r2_scores)//2]\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "    \n",
                "    for ax_idx, (result_idx, title) in enumerate([\n",
                "        (best_idx, f\"Best Model (R² = {validation_results[best_idx]['r2']:.3f})\"),\n",
                "        (median_idx, f\"Median Model (R² = {validation_results[median_idx]['r2']:.3f})\"),\n",
                "        (worst_idx, f\"Worst Model (R² = {validation_results[worst_idx]['r2']:.3f})\")\n",
                "    ]):\n",
                "        result = validation_results[result_idx]\n",
                "        actual = result['actual']\n",
                "        predicted = result['predicted']\n",
                "        \n",
                "        months_test = [f\"{months[i]}\" for i in range(TRAIN_MONTHS, TRAIN_MONTHS+TEST_MONTHS)]\n",
                "        \n",
                "        axes[ax_idx].plot(months_test, actual, 'o-', label='Actual', linewidth=2, markersize=6)\n",
                "        axes[ax_idx].plot(months_test, predicted, 's--', label='Predicted', linewidth=2, markersize=6)\n",
                "        axes[ax_idx].set_title(title, fontsize=12, fontweight='bold')\n",
                "        axes[ax_idx].set_xlabel('Month', fontsize=10)\n",
                "        axes[ax_idx].set_ylabel('HSI', fontsize=10)\n",
                "        axes[ax_idx].legend(fontsize=9)\n",
                "        axes[ax_idx].grid(True, alpha=0.3)\n",
                "        axes[ax_idx].tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../data/predictions/validation_plots.png', dpi=150, bbox_inches='tight')\n",
                "    print(\"✅ Validation plots saved to data/predictions/validation_plots.png\")\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"⚠️  No validation results to plot\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train ARIMA Models for All Grid Points (Full Dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from multiprocessing import Pool, cpu_count\n",
                "\n",
                "def train_arima_for_point(args):\n",
                "    \"\"\"\n",
                "    Train ARIMA model for a single grid point (optimized)\n",
                "    \"\"\"\n",
                "    i, j, time_series = args\n",
                "    \n",
                "    try:\n",
                "        if np.all(np.isnan(time_series)) or np.nanstd(time_series) == 0:\n",
                "            return (i, j, None, None)\n",
                "        \n",
                "        # Optimized ARIMA - reduced max parameters untuk speed\n",
                "        model = auto_arima(\n",
                "            time_series,\n",
                "            start_p=0, start_q=0,\n",
                "            max_p=3, max_q=3,  # Reduced dari 5\n",
                "            seasonal=False,\n",
                "            stepwise=True,\n",
                "            suppress_warnings=True,\n",
                "            error_action='ignore',\n",
                "            max_order=6,  # Reduced dari 10\n",
                "            n_jobs=1\n",
                "        )\n",
                "        \n",
                "        return (i, j, model, model.order)\n",
                "    except Exception as e:\n",
                "        return (i, j, None, None)\n",
                "\n",
                "print(\"✅ ARIMA training function defined (optimized)!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for training\n",
                "print(\"=== Training ARIMA Models (Full Dataset) ===\")\n",
                "print(f\"Training {n_lat * n_lon} models (one per grid point)...\")\n",
                "\n",
                "training_data = []\n",
                "for i in range(n_lat):\n",
                "    for j in range(n_lon):\n",
                "        time_series = hsi_processed[:, i, j]\n",
                "        training_data.append((i, j, time_series))\n",
                "\n",
                "print(f\"Total grid points: {len(training_data)}\")\n",
                "\n",
                "# Train models (with parallel processing)\n",
                "USE_PARALLEL = True\n",
                "N_WORKERS = min(4, cpu_count())\n",
                "\n",
                "start_time = time.time()\n",
                "trained_models = {}\n",
                "\n",
                "if USE_PARALLEL and N_WORKERS > 1:\n",
                "    print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
                "    with Pool(processes=N_WORKERS) as pool:\n",
                "        results = pool.map(train_arima_for_point, training_data)\n",
                "    \n",
                "    for i, j, model, order in results:\n",
                "        if model is not None:\n",
                "            trained_models[(i, j)] = model\n",
                "else:\n",
                "    print(\"Using sequential processing...\")\n",
                "    for idx, args in enumerate(training_data):\n",
                "        i, j, model, order = train_arima_for_point(args)\n",
                "        if model is not None:\n",
                "            trained_models[(i, j)] = model\n",
                "        \n",
                "        if (idx + 1) % 20 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
                "            remaining = (len(training_data) - idx - 1) / rate if rate > 0 else 0\n",
                "            print(f\"  Progress: {idx+1}/{len(training_data)} ({100*(idx+1)/len(training_data):.1f}%) | \"\n",
                "                  f\"Elapsed: {elapsed:.1f}s | ETA: {remaining:.1f}s\")\n",
                "\n",
                "elapsed = time.time() - start_time\n",
                "print(f\"\\n✅ Model training complete in {elapsed:.1f}s ({elapsed/60:.1f} minutes)!\")\n",
                "print(f\"Successfully trained: {len(trained_models)}/{len(training_data)} models\")\n",
                "print(f\"Failed: {len(training_data) - len(trained_models)} models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Predict HSI for Future Years (2025-2030)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict for years 2025-2030\n",
                "PREDICTION_YEARS = [2025, 2026, 2027, 2028, 2029, 2030]\n",
                "MONTHS_PER_YEAR = 12\n",
                "\n",
                "print(f\"=== Predicting HSI for {PREDICTION_YEARS} ===\")\n",
                "\n",
                "# Store predictions: {year: [n_lat, n_lon] array}\n",
                "predictions = {}\n",
                "\n",
                "for year in PREDICTION_YEARS:\n",
                "    print(f\"\\nPredicting for year {year}...\")\n",
                "    \n",
                "    year_index = year - 2025\n",
                "    start_month_idx = 48 + (year_index * 12)\n",
                "    n_steps = 12  # Predict 12 months per year\n",
                "    \n",
                "    prediction_array = np.full((n_lat, n_lon), np.nan)\n",
                "    \n",
                "    for (i, j), model in trained_models.items():\n",
                "        try:\n",
                "            # Predict 12 months ahead\n",
                "            forecast = model.predict(n_periods=n_steps)\n",
                "            \n",
                "            # Average to get annual value\n",
                "            annual_value = np.mean(forecast)\n",
                "            \n",
                "            # Clip to valid range [0, 1]\n",
                "            annual_value = np.clip(annual_value, 0.0, 1.0)\n",
                "            \n",
                "            prediction_array[i, j] = annual_value\n",
                "        except Exception as e:\n",
                "            continue\n",
                "    \n",
                "    predictions[year] = prediction_array\n",
                "    print(f\"  ✅ Year {year}: {np.sum(~np.isnan(prediction_array))} valid predictions\")\n",
                "\n",
                "print(f\"\\n✅ Prediction complete for {len(PREDICTION_YEARS)} years!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Generate GeoJSON for Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_prediction_geojson(year, prediction_array, lat_grid, lon_grid):\n",
                "    \"\"\"\n",
                "    Create GeoJSON from prediction array\n",
                "    \"\"\"\n",
                "    features = []\n",
                "    \n",
                "    # Create meshgrid\n",
                "    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
                "    \n",
                "    for i in range(len(lat_grid)):\n",
                "        for j in range(len(lon_grid)):\n",
                "            lat = lat_mesh[i, j]\n",
                "            lon = lon_mesh[i, j]\n",
                "            hsi_val = float(prediction_array[i, j]) if not np.isnan(prediction_array[i, j]) else None\n",
                "            \n",
                "            if hsi_val is None:\n",
                "                continue\n",
                "            \n",
                "            feature = {\n",
                "                \"type\": \"Feature\",\n",
                "                \"geometry\": {\n",
                "                    \"type\": \"Point\",\n",
                "                    \"coordinates\": [float(lon), float(lat)]\n",
                "                },\n",
                "                \"properties\": {\n",
                "                    \"hsi\": hsi_val,\n",
                "                    \"year\": year,\n",
                "                    \"is_prediction\": True\n",
                "                }\n",
                "            }\n",
                "            \n",
                "            features.append(feature)\n",
                "    \n",
                "    geojson = {\n",
                "        \"type\": \"FeatureCollection\",\n",
                "        \"features\": features\n",
                "    }\n",
                "    \n",
                "    return geojson\n",
                "\n",
                "print(\"✅ GeoJSON creation function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate GeoJSON files for each prediction year\n",
                "OUTPUT_DIR = '../data/predictions'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"=== Generating GeoJSON Files ===\")\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")\n",
                "\n",
                "prediction_files = []\n",
                "\n",
                "for year in PREDICTION_YEARS:\n",
                "    prediction_array = predictions[year]\n",
                "    geojson = create_prediction_geojson(year, prediction_array, lat_grid, lon_grid)\n",
                "    \n",
                "    # Save GeoJSON\n",
                "    filename = f\"prediction_{year}.geojson\"\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    \n",
                "    with open(filepath, 'w', encoding='utf-8') as f:\n",
                "        json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    prediction_files.append({\n",
                "        'year': year,\n",
                "        'file': filename,\n",
                "        'features': len(geojson['features'])\n",
                "    })\n",
                "    \n",
                "    print(f\"  ✅ Saved {filename} ({len(geojson['features'])} features)\")\n",
                "\n",
                "print(f\"\\n✅ Generated {len(prediction_files)} prediction files!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Generate Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create metadata for predictions\n",
                "metadata = {\n",
                "    \"title\": \"HSI Predictions for Sunda Strait\",\n",
                "    \"description\": \"Predicted Habitat Suitability Index values for future years using ARIMA model\",\n",
                "    \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "    \"model_type\": \"ARIMA\",\n",
                "    \"grid_resolution\": TARGET_RESOLUTION,\n",
                "    \"validation\": {\n",
                "        \"method\": \"walk_forward\",\n",
                "        \"train_period\": f\"{months[0]} to {months[TRAIN_MONTHS-1]}\",\n",
                "        \"test_period\": f\"{months[TRAIN_MONTHS]} to {months[TRAIN_MONTHS+TEST_MONTHS-1]}\",\n",
                "        \"sample_points\": VALIDATION_SAMPLE,\n",
                "        \"metrics\": {\n",
                "            \"mean_r2\": float(np.mean([r['r2'] for r in validation_results])) if validation_results else None,\n",
                "            \"mean_mae\": float(np.mean([r['mae'] for r in validation_results])) if validation_results else None,\n",
                "            \"mean_rmse\": float(np.mean([r['rmse'] for r in validation_results])) if validation_results else None,\n",
                "            \"mean_mape\": float(np.mean([r['mape'] for r in validation_results])) if validation_results else None\n",
                "        }\n",
                "    },\n",
                "    \"training_data\": {\n",
                "        \"start\": months[0],\n",
                "        \"end\": months[-1],\n",
                "        \"total_months\": len(months)\n",
                "    },\n",
                "    \"prediction_years\": PREDICTION_YEARS,\n",
                "    \"spatial_bounds\": {\n",
                "        \"min_lat\": float(lat_grid.min()),\n",
                "        \"max_lat\": float(lat_grid.max()),\n",
                "        \"min_lon\": float(lon_grid.min()),\n",
                "        \"max_lon\": float(lon_grid.max())\n",
                "    },\n",
                "    \"available_predictions\": prediction_files,\n",
                "    \"total_predictions\": len(prediction_files),\n",
                "    \"models_trained\": len(trained_models),\n",
                "    \"note\": \"Predictions are based on historical data (2021-2024) and should be used as estimates only.\"\n",
                "}\n",
                "\n",
                "# Save metadata\n",
                "metadata_file = os.path.join(OUTPUT_DIR, 'metadata.json')\n",
                "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
                "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"✅ Metadata saved to {metadata_file}\")\n",
                "print(f\"\\nMetadata summary:\")\n",
                "print(f\"  Grid resolution: {TARGET_RESOLUTION}°\")\n",
                "print(f\"  Training data: {metadata['training_data']['start']} to {metadata['training_data']['end']}\")\n",
                "print(f\"  Prediction years: {metadata['prediction_years']}\")\n",
                "print(f\"  Models trained: {metadata['models_trained']}\")\n",
                "if validation_results:\n",
                "    print(f\"  Validation R²: {metadata['validation']['metrics']['mean_r2']:.4f}\")\n",
                "    print(f\"  Validation MAE: {metadata['validation']['metrics']['mean_mae']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== PREDICTION MODEL SUMMARY ===\")\n",
                "print(\"\\n✅ Prediction model training and prediction completed!\")\n",
                "print(\"\\nWhat was done:\")\n",
                "print(\"1. ✅ Loaded monthly HSI data (2021-2024)\")\n",
                "print(f\"2. ✅ Reduced grid resolution to {TARGET_RESOLUTION}° (optimization)\")\n",
                "print(\"3. ✅ Preprocessed data (interpolated missing values)\")\n",
                "print(\"4. ✅ Walk-forward validation with all metrics\")\n",
                "print(\"5. ✅ Trained ARIMA models per grid point\")\n",
                "print(f\"6. ✅ Predicted HSI for {len(PREDICTION_YEARS)} years ({PREDICTION_YEARS[0]}-{PREDICTION_YEARS[-1]})\")\n",
                "print(\"7. ✅ Generated GeoJSON files for predictions\")\n",
                "print(\"8. ✅ Created metadata with validation metrics\")\n",
                "print(\"\\nOutput files:\")\n",
                "print(f\"  - {len(prediction_files)} prediction GeoJSON files in data/predictions/\")\n",
                "print(f\"  - metadata.json with prediction and validation information\")\n",
                "print(f\"  - validation_plots.png with actual vs predicted plots\")\n",
                "print(\"\\nNext Steps:\")\n",
                "print(\"- Backend: Create API endpoints for predictions\")\n",
                "print(\"- Frontend: Integrate prediction visualization\")\n",
                "print(\"\\n✅ Jupyter prediction phase complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write notebook to file
with open('07_prediction_model.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print("Optimized prediction notebook created successfully!")

