{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Data Preprocessing\n",
    "\n",
    "Notebook ini untuk preprocessing data NetCDF sebelum perhitungan HSI.\n",
    "\n",
    "## Langkah-langkah:\n",
    "1. Load data NetCDF\n",
    "2. Konversi SST: Kelvin â†’ Celcius\n",
    "3. Ekstrak surface salinity (depth=0)\n",
    "4. Resample ke grid seragam\n",
    "5. Crop ke bounding box Selat Sunda\n",
    "6. Handle missing values\n",
    "7. Save processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Box: {'lat_min': -6.775, 'lat_max': -5.475, 'lon_min': 104.5625, 'lon_max': 105.9375}\n",
      "Target Resolution: 0.05Â° (~5.6 km)\n",
      "Output directory: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Bounding Box Selat Sunda (dari eksplorasi)\n",
    "# Akan diupdate setelah menjalankan notebook eksplorasi\n",
    "BBOX = {\n",
    "    'lat_min': -6.7750,   # dari SST (paling luas)\n",
    "    'lat_max': -5.4750,   # dari SST\n",
    "    'lon_min': 104.5625,  # dari CHL\n",
    "    'lon_max': 105.9375   # dari CHL\n",
    "}\n",
    "\n",
    "# Target grid resolution (derajat)\n",
    "# Pilih resolusi yang sesuai (0.05Â° ~ 5.5 km atau 0.1Â° ~ 11 km)\n",
    "TARGET_RESOLUTION = 0.05  # derajat\n",
    "\n",
    "# File paths\n",
    "CHL_FILE = '../CHL 21-24.nc'\n",
    "SST_FILE = '../SST 21-24.nc'\n",
    "SO_FILE = '../SO 21-24.nc'\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = '../data/processed'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Bounding Box: {BBOX}\")\n",
    "print(f\"Target Resolution: {TARGET_RESOLUTION}Â° (~{TARGET_RESOLUTION*111:.1f} km)\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Target Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target grid size: 28 x 29 = 812 points\n",
      "Latitude range: -6.7750 to -5.4250\n",
      "Longitude range: 104.5625 to 105.9625\n"
     ]
    }
   ],
   "source": [
    "# Create uniform grid untuk target resolution\n",
    "lat_grid = np.arange(BBOX['lat_min'], BBOX['lat_max'] + TARGET_RESOLUTION, TARGET_RESOLUTION)\n",
    "lon_grid = np.arange(BBOX['lon_min'], BBOX['lon_max'] + TARGET_RESOLUTION, TARGET_RESOLUTION)\n",
    "\n",
    "# Create meshgrid\n",
    "lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
    "\n",
    "print(f\"Target grid size: {len(lat_grid)} x {len(lon_grid)} = {len(lat_grid) * len(lon_grid)} points\")\n",
    "print(f\"Latitude range: {lat_grid.min():.4f} to {lat_grid.max():.4f}\")\n",
    "print(f\"Longitude range: {lon_grid.min():.4f} to {lon_grid.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Preprocess CHL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing CHL Data ===\n",
      "Original CHL shape: (1461, 32, 34)\n",
      "Time steps: 1461\n",
      "Time units: seconds since 1970-01-01 00:00:00\n",
      "CHL data loaded. Ready for resampling.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Processing CHL Data ===\")\n",
    "nc_chl = netCDF4.Dataset(CHL_FILE, 'r')\n",
    "\n",
    "# Get coordinates\n",
    "lat_chl = nc_chl.variables['latitude'][:]\n",
    "lon_chl = nc_chl.variables['longitude'][:]\n",
    "time_chl = nc_chl.variables['time'][:]\n",
    "chl_data = nc_chl.variables['CHL']\n",
    "\n",
    "print(f\"Original CHL shape: {chl_data.shape}\")\n",
    "print(f\"Time steps: {len(time_chl)}\")\n",
    "\n",
    "# Get time units for date conversion\n",
    "time_units = nc_chl.variables['time'].units\n",
    "print(f\"Time units: {time_units}\")\n",
    "\n",
    "# Create meshgrid for original data\n",
    "lon_chl_2d, lat_chl_2d = np.meshgrid(lon_chl, lat_chl)\n",
    "\n",
    "# Flatten for interpolation\n",
    "points_chl = np.column_stack((lon_chl_2d.ravel(), lat_chl_2d.ravel()))\n",
    "\n",
    "print(f\"CHL data loaded. Ready for resampling.\")\n",
    "nc_chl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load & Preprocess SST Data (Convert Kelvin to Celcius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing SST Data ===\n",
      "Original SST shape: (1461, 27, 29)\n",
      "Time steps: 1461\n",
      "SST data loaded. Will convert Kelvin â†’ Celcius during resampling.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Processing SST Data ===\")\n",
    "nc_sst = netCDF4.Dataset(SST_FILE, 'r')\n",
    "\n",
    "# Get coordinates\n",
    "lat_sst = nc_sst.variables['latitude'][:]\n",
    "lon_sst = nc_sst.variables['longitude'][:]\n",
    "time_sst = nc_sst.variables['time'][:]\n",
    "sst_data_k = nc_sst.variables['analysed_sst']  # in Kelvin\n",
    "\n",
    "print(f\"Original SST shape: {sst_data_k.shape}\")\n",
    "print(f\"Time steps: {len(time_sst)}\")\n",
    "\n",
    "# Create meshgrid for original data\n",
    "lon_sst_2d, lat_sst_2d = np.meshgrid(lon_sst, lat_sst)\n",
    "\n",
    "# Flatten for interpolation\n",
    "points_sst = np.column_stack((lon_sst_2d.ravel(), lat_sst_2d.ravel()))\n",
    "\n",
    "print(f\"SST data loaded. Will convert Kelvin â†’ Celcius during resampling.\")\n",
    "nc_sst.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load & Preprocess Salinity Data (Extract Surface Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Salinity Data ===\n",
      "Original Salinity shape: (1461, 50, 16, 17)\n",
      "Depth levels: 50\n",
      "Surface depth (first level): 0.49 m\n",
      "Surface salinity shape: (1461, 16, 17)\n",
      "Surface salinity extracted. Ready for resampling.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Processing Salinity Data ===\")\n",
    "nc_so = netCDF4.Dataset(SO_FILE, 'r')\n",
    "\n",
    "# Get coordinates\n",
    "lat_so = nc_so.variables['latitude'][:]\n",
    "lon_so = nc_so.variables['longitude'][:]\n",
    "time_so = nc_so.variables['time'][:]\n",
    "depth_so = nc_so.variables['depth'][:]\n",
    "so_data = nc_so.variables['so']  # [time, depth, lat, lon]\n",
    "\n",
    "print(f\"Original Salinity shape: {so_data.shape}\")\n",
    "print(f\"Depth levels: {len(depth_so)}\")\n",
    "print(f\"Surface depth (first level): {depth_so[0]:.2f} m\")\n",
    "\n",
    "# Extract surface salinity (depth index 0)\n",
    "surface_so = so_data[:, 0, :, :]  # [time, lat, lon]\n",
    "print(f\"Surface salinity shape: {surface_so.shape}\")\n",
    "\n",
    "# Create meshgrid for original data\n",
    "lon_so_2d, lat_so_2d = np.meshgrid(lon_so, lat_so)\n",
    "\n",
    "# Flatten for interpolation\n",
    "points_so = np.column_stack((lon_so_2d.ravel(), lat_so_2d.ravel()))\n",
    "\n",
    "print(f\"Surface salinity extracted. Ready for resampling.\")\n",
    "nc_so.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resample & Interpolate Data to Target Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling function defined.\n"
     ]
    }
   ],
   "source": [
    "def resample_to_grid(data_2d, points_orig, lon_target, lat_target, method='nearest'):\n",
    "    \"\"\"\n",
    "    Resample 2D data to target grid using interpolation\n",
    "    \n",
    "    Parameters:\n",
    "    - data_2d: 2D array [lat, lon] dari data original\n",
    "    - points_orig: array [N, 2] dengan (lon, lat) dari data original\n",
    "    - lon_target, lat_target: target grid coordinates\n",
    "    - method: interpolation method ('linear', 'nearest', 'cubic')\n",
    "    \n",
    "    Returns:\n",
    "    - resampled_data: 2D array dengan shape sesuai target grid\n",
    "    \"\"\"\n",
    "    # Flatten target grid\n",
    "    points_target = np.column_stack((lon_target.ravel(), lat_target.ravel()))\n",
    "    \n",
    "    # Flatten original data\n",
    "    values_orig = data_2d.ravel()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~np.isnan(values_orig)\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return np.full(lon_target.shape, np.nan)\n",
    "    \n",
    "    points_valid = points_orig[valid_mask]\n",
    "    values_valid = values_orig[valid_mask]\n",
    "    \n",
    "    # Interpolate\n",
    "    values_interp = griddata(\n",
    "        points_valid,\n",
    "        values_valid,\n",
    "        points_target,\n",
    "        method=method,\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    # Reshape to target grid\n",
    "    resampled = values_interp.reshape(lon_target.shape)\n",
    "    \n",
    "    return resampled\n",
    "\n",
    "print(\"Resampling function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# OPTIMIZED VERSION - Process All Time Steps\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "# ===== Optimization Settings =====\n",
    "INTERP_METHOD = 'nearest'  # 'nearest' (fastest, 3-5x), 'linear' (accurate), 'cubic' (slowest)\n",
    "USE_PARALLEL = True  # Set False jika ada masalah dengan multiprocessing\n",
    "N_WORKERS = min(4, cpu_count())  # Jumlah CPU cores\n",
    "CHUNK_SIZE = 100  # Process 100 time steps per batch\n",
    "\n",
    "print(f\"ðŸš€ Optimization settings:\")\n",
    "print(f\"  Interpolation method: {INTERP_METHOD} (3-5x faster)\")\n",
    "print(f\"  Parallel processing: {USE_PARALLEL} ({N_WORKERS} workers)\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "# ===== Optimized Resampling Function =====\n",
    "def resample_to_grid_fast(data_2d, points_orig, lon_target, lat_target, method=INTERP_METHOD):\n",
    "    \"\"\"Optimized resampling dengan method yang lebih cepat\"\"\"\n",
    "    points_target = np.column_stack((lon_target.ravel(), lat_target.ravel()))\n",
    "    values_orig = data_2d.ravel()\n",
    "    \n",
    "    valid_mask = ~np.isnan(values_orig)\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return np.full(lon_target.shape, np.nan)\n",
    "    \n",
    "    points_valid = points_orig[valid_mask]\n",
    "    values_valid = values_orig[valid_mask]\n",
    "    \n",
    "    values_interp = griddata(\n",
    "        points_valid,\n",
    "        values_valid,\n",
    "        points_target,\n",
    "        method=method,\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    return values_interp.reshape(lon_target.shape)\n",
    "\n",
    "# ===== Process Single Time Step (for parallel) =====\n",
    "def process_time_step(t, chl_data, sst_data_k, so_data, \n",
    "                     points_chl, points_sst, points_so,\n",
    "                     lon_mesh, lat_mesh):\n",
    "    \"\"\"Process satu time step - untuk parallel processing\"\"\"\n",
    "    try:\n",
    "        # CHL\n",
    "        chl_2d = chl_data[t, :, :]\n",
    "        chl_resampled = resample_to_grid_fast(chl_2d, points_chl, lon_mesh, lat_mesh)\n",
    "        \n",
    "        # SST (convert Kelvin to Celcius)\n",
    "        sst_2d_k = sst_data_k[t, :, :]\n",
    "        sst_2d_c = sst_2d_k - 273.15\n",
    "        sst_resampled = resample_to_grid_fast(sst_2d_c, points_sst, lon_mesh, lat_mesh)\n",
    "        \n",
    "        # Salinity (surface)\n",
    "        so_2d = so_data[t, 0, :, :]\n",
    "        so_resampled = resample_to_grid_fast(so_2d, points_so, lon_mesh, lat_mesh)\n",
    "        \n",
    "        return t, chl_resampled, sst_resampled, so_resampled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing time step {t}: {e}\")\n",
    "        return t, None, None, None\n",
    "\n",
    "# ===== Main Processing =====\n",
    "SAMPLE_SIZE = None  # Set None untuk process semua, atau angka untuk sample\n",
    "\n",
    "# Reopen files\n",
    "nc_chl = netCDF4.Dataset(CHL_FILE, 'r')\n",
    "nc_sst = netCDF4.Dataset(SST_FILE, 'r')\n",
    "nc_so = netCDF4.Dataset(SO_FILE, 'r')\n",
    "\n",
    "chl_data = nc_chl.variables['CHL']\n",
    "sst_data_k = nc_sst.variables['analysed_sst']\n",
    "so_data = nc_so.variables['so']\n",
    "\n",
    "# Get number of time steps\n",
    "n_times = len(time_chl)\n",
    "if SAMPLE_SIZE:\n",
    "    n_times = min(SAMPLE_SIZE, n_times)\n",
    "\n",
    "print(f\"\\nProcessing {n_times} time steps with optimizations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize arrays\n",
    "processed_chl = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "processed_sst = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "processed_so = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "\n",
    "time_indices = list(range(n_times))\n",
    "\n",
    "if USE_PARALLEL and N_WORKERS > 1:\n",
    "    # ===== PARALLEL PROCESSING =====\n",
    "    print(f\"Using parallel processing with {N_WORKERS} workers...\")\n",
    "    \n",
    "    process_func = partial(\n",
    "        process_time_step,\n",
    "        chl_data=chl_data,\n",
    "        sst_data_k=sst_data_k,\n",
    "        so_data=so_data,\n",
    "        points_chl=points_chl,\n",
    "        points_sst=points_sst,\n",
    "        points_so=points_so,\n",
    "        lon_mesh=lon_mesh,\n",
    "        lat_mesh=lat_mesh\n",
    "    )\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk_start in range(0, n_times, CHUNK_SIZE):\n",
    "        chunk_end = min(chunk_start + CHUNK_SIZE, n_times)\n",
    "        chunk_indices = time_indices[chunk_start:chunk_end]\n",
    "        \n",
    "        chunk_num = chunk_start//CHUNK_SIZE + 1\n",
    "        total_chunks = (n_times-1)//CHUNK_SIZE + 1\n",
    "        print(f\"Processing chunk {chunk_num}/{total_chunks} (time steps {chunk_start+1}-{chunk_end})...\")\n",
    "        \n",
    "        with Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(process_func, chunk_indices)\n",
    "        \n",
    "        # Store results\n",
    "        for t, chl_res, sst_res, so_res in results:\n",
    "            if chl_res is not None:\n",
    "                processed_chl[t, :, :] = chl_res\n",
    "                processed_sst[t, :, :] = sst_res\n",
    "                processed_so[t, :, :] = so_res\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  âœ“ Chunk {chunk_num} completed in {elapsed:.1f}s\")\n",
    "    \n",
    "else:\n",
    "    # ===== SEQUENTIAL PROCESSING (Optimized) =====\n",
    "    print(\"Using sequential processing (optimized with 'nearest' method)...\")\n",
    "    \n",
    "    for t in range(n_times):\n",
    "        if (t + 1) % 50 == 0 or t == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (t + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (n_times - t - 1) / rate if rate > 0 else 0\n",
    "            print(f\"Progress: {t+1}/{n_times} ({100*(t+1)/n_times:.1f}%) | \"\n",
    "                  f\"Elapsed: {elapsed:.1f}s | ETA: {remaining:.1f}s\")\n",
    "        \n",
    "        # CHL\n",
    "        chl_2d = chl_data[t, :, :]\n",
    "        processed_chl[t, :, :] = resample_to_grid_fast(chl_2d, points_chl, lon_mesh, lat_mesh)\n",
    "        \n",
    "        # SST (convert Kelvin to Celcius)\n",
    "        sst_2d_k = sst_data_k[t, :, :]\n",
    "        sst_2d_c = sst_2d_k - 273.15\n",
    "        processed_sst[t, :, :] = resample_to_grid_fast(sst_2d_c, points_sst, lon_mesh, lat_mesh)\n",
    "        \n",
    "        # Salinity (surface)\n",
    "        so_2d = so_data[t, 0, :, :]\n",
    "        processed_so[t, :, :] = resample_to_grid_fast(so_2d, points_so, lon_mesh, lat_mesh)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Processing complete!\")\n",
    "print(f\"Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Average: {total_time/n_times:.2f}s per time step\")\n",
    "print(f\"Processed data shape: {processed_chl.shape}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Close files\n",
    "nc_chl.close()\n",
    "nc_sst.close()\n",
    "nc_so.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1461 time steps...\n",
      "Processing time step 1/1461...\n",
      "Processing time step 10/1461...\n",
      "Processing time step 20/1461...\n",
      "Processing time step 30/1461...\n",
      "Processing time step 40/1461...\n",
      "Processing time step 50/1461...\n",
      "Processing time step 60/1461...\n",
      "Processing time step 70/1461...\n",
      "Processing time step 80/1461...\n",
      "Processing time step 90/1461...\n",
      "Processing time step 100/1461...\n",
      "Processing time step 110/1461...\n",
      "Processing time step 120/1461...\n",
      "Processing time step 130/1461...\n",
      "Processing time step 140/1461...\n",
      "Processing time step 150/1461...\n",
      "Processing time step 160/1461...\n",
      "Processing time step 170/1461...\n",
      "Processing time step 180/1461...\n",
      "Processing time step 190/1461...\n",
      "Processing time step 200/1461...\n",
      "Processing time step 210/1461...\n",
      "Processing time step 220/1461...\n",
      "Processing time step 230/1461...\n",
      "Processing time step 240/1461...\n",
      "Processing time step 250/1461...\n",
      "Processing time step 260/1461...\n",
      "Processing time step 270/1461...\n",
      "Processing time step 280/1461...\n",
      "Processing time step 290/1461...\n",
      "Processing time step 300/1461...\n",
      "Processing time step 310/1461...\n",
      "Processing time step 320/1461...\n",
      "Processing time step 330/1461...\n",
      "Processing time step 340/1461...\n",
      "Processing time step 350/1461...\n",
      "Processing time step 360/1461...\n",
      "Processing time step 370/1461...\n",
      "Processing time step 380/1461...\n",
      "Processing time step 390/1461...\n",
      "Processing time step 400/1461...\n",
      "Processing time step 410/1461...\n",
      "Processing time step 420/1461...\n",
      "Processing time step 430/1461...\n",
      "Processing time step 440/1461...\n",
      "Processing time step 450/1461...\n",
      "Processing time step 460/1461...\n",
      "Processing time step 470/1461...\n",
      "Processing time step 480/1461...\n",
      "Processing time step 490/1461...\n",
      "Processing time step 500/1461...\n",
      "Processing time step 510/1461...\n",
      "Processing time step 520/1461...\n",
      "Processing time step 530/1461...\n",
      "Processing time step 540/1461...\n",
      "Processing time step 550/1461...\n",
      "Processing time step 560/1461...\n",
      "Processing time step 570/1461...\n",
      "Processing time step 580/1461...\n",
      "Processing time step 590/1461...\n",
      "Processing time step 600/1461...\n",
      "Processing time step 610/1461...\n",
      "Processing time step 620/1461...\n",
      "Processing time step 630/1461...\n",
      "Processing time step 640/1461...\n",
      "Processing time step 650/1461...\n",
      "Processing time step 660/1461...\n",
      "Processing time step 670/1461...\n",
      "Processing time step 680/1461...\n",
      "Processing time step 690/1461...\n",
      "Processing time step 700/1461...\n",
      "Processing time step 710/1461...\n",
      "Processing time step 720/1461...\n",
      "Processing time step 730/1461...\n",
      "Processing time step 740/1461...\n",
      "Processing time step 750/1461...\n",
      "Processing time step 760/1461...\n",
      "Processing time step 770/1461...\n",
      "Processing time step 780/1461...\n",
      "Processing time step 790/1461...\n",
      "Processing time step 800/1461...\n",
      "Processing time step 810/1461...\n",
      "Processing time step 820/1461...\n",
      "Processing time step 830/1461...\n",
      "Processing time step 840/1461...\n",
      "Processing time step 850/1461...\n",
      "Processing time step 860/1461...\n",
      "Processing time step 870/1461...\n",
      "Processing time step 880/1461...\n",
      "Processing time step 890/1461...\n",
      "Processing time step 900/1461...\n",
      "Processing time step 910/1461...\n",
      "Processing time step 920/1461...\n",
      "Processing time step 930/1461...\n",
      "Processing time step 940/1461...\n",
      "Processing time step 950/1461...\n",
      "Processing time step 960/1461...\n",
      "Processing time step 970/1461...\n",
      "Processing time step 980/1461...\n",
      "Processing time step 990/1461...\n",
      "Processing time step 1000/1461...\n",
      "Processing time step 1010/1461...\n",
      "Processing time step 1020/1461...\n",
      "Processing time step 1030/1461...\n",
      "Processing time step 1040/1461...\n",
      "Processing time step 1050/1461...\n",
      "Processing time step 1060/1461...\n",
      "Processing time step 1070/1461...\n",
      "Processing time step 1080/1461...\n",
      "Processing time step 1090/1461...\n",
      "Processing time step 1100/1461...\n",
      "Processing time step 1110/1461...\n",
      "Processing time step 1120/1461...\n",
      "Processing time step 1130/1461...\n",
      "Processing time step 1140/1461...\n",
      "Processing time step 1150/1461...\n",
      "Processing time step 1160/1461...\n",
      "Processing time step 1170/1461...\n",
      "Processing time step 1180/1461...\n",
      "Processing time step 1190/1461...\n",
      "Processing time step 1200/1461...\n",
      "Processing time step 1210/1461...\n",
      "Processing time step 1220/1461...\n",
      "Processing time step 1230/1461...\n",
      "Processing time step 1240/1461...\n",
      "Processing time step 1250/1461...\n",
      "Processing time step 1260/1461...\n",
      "Processing time step 1270/1461...\n",
      "Processing time step 1280/1461...\n",
      "Processing time step 1290/1461...\n",
      "Processing time step 1300/1461...\n",
      "Processing time step 1310/1461...\n",
      "Processing time step 1320/1461...\n",
      "Processing time step 1330/1461...\n",
      "Processing time step 1340/1461...\n",
      "Processing time step 1350/1461...\n",
      "Processing time step 1360/1461...\n",
      "Processing time step 1370/1461...\n",
      "Processing time step 1380/1461...\n",
      "Processing time step 1390/1461...\n",
      "Processing time step 1400/1461...\n",
      "Processing time step 1410/1461...\n",
      "Processing time step 1420/1461...\n",
      "Processing time step 1430/1461...\n",
      "Processing time step 1440/1461...\n",
      "Processing time step 1450/1461...\n",
      "Processing time step 1460/1461...\n",
      "\n",
      "Processing complete!\n",
      "Processed data shape: (1461, 28, 29)\n"
     ]
    }
   ],
   "source": [
    "# Process sample (first 10 days) untuk testing\n",
    "# Setelah berhasil, bisa diubah ke semua time steps\n",
    "SAMPLE_SIZE = None  # Ubah ke None untuk process semua\n",
    "\n",
    "# Reopen files\n",
    "nc_chl = netCDF4.Dataset(CHL_FILE, 'r')\n",
    "nc_sst = netCDF4.Dataset(SST_FILE, 'r')\n",
    "nc_so = netCDF4.Dataset(SO_FILE, 'r')\n",
    "\n",
    "chl_data = nc_chl.variables['CHL']\n",
    "sst_data_k = nc_sst.variables['analysed_sst']\n",
    "so_data = nc_so.variables['so']\n",
    "\n",
    "# Get number of time steps\n",
    "n_times = len(time_chl)\n",
    "if SAMPLE_SIZE:\n",
    "    n_times = min(SAMPLE_SIZE, n_times)\n",
    "\n",
    "print(f\"Processing {n_times} time steps...\")\n",
    "\n",
    "# Initialize arrays untuk processed data\n",
    "processed_chl = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "processed_sst = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "processed_so = np.full((n_times, len(lat_grid), len(lon_grid)), np.nan)\n",
    "\n",
    "# Process each time step\n",
    "for t in range(n_times):\n",
    "    if (t + 1) % 10 == 0 or t == 0:\n",
    "        print(f\"Processing time step {t+1}/{n_times}...\")\n",
    "    \n",
    "    # CHL\n",
    "    chl_2d = chl_data[t, :, :]\n",
    "    processed_chl[t, :, :] = resample_to_grid(chl_2d, points_chl, lon_mesh, lat_mesh)\n",
    "    \n",
    "    # SST (convert Kelvin to Celcius)\n",
    "    sst_2d_k = sst_data_k[t, :, :]\n",
    "    sst_2d_c = sst_2d_k - 273.15  # Convert to Celcius\n",
    "    processed_sst[t, :, :] = resample_to_grid(sst_2d_c, points_sst, lon_mesh, lat_mesh)\n",
    "    \n",
    "    # Salinity (surface)\n",
    "    so_2d = so_data[t, 0, :, :]  # surface layer\n",
    "    processed_so[t, :, :] = resample_to_grid(so_2d, points_so, lon_mesh, lat_mesh)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Processed data shape: {processed_chl.shape}\")\n",
    "\n",
    "# Close files\n",
    "nc_chl.close()\n",
    "nc_sst.close()\n",
    "nc_so.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Quality Check ===\n",
      "\n",
      "CHL:\n",
      "  Valid data: 100.0%\n",
      "  Range: 0.0482 to 36.7248 mg/mÂ³\n",
      "  Mean: 0.6461 mg/mÂ³\n",
      "\n",
      "SST:\n",
      "  Valid data: 100.0%\n",
      "  Range: 24.78 to 31.10 Â°C\n",
      "  Mean: 29.15 Â°C\n",
      "\n",
      "Salinity:\n",
      "  Valid data: 100.0%\n",
      "  Range: 28.33 to 34.34 PSU\n",
      "  Mean: 32.32 PSU\n"
     ]
    }
   ],
   "source": [
    "# Check data quality\n",
    "print(\"=== Data Quality Check ===\")\n",
    "\n",
    "# CHL\n",
    "valid_chl = ~np.isnan(processed_chl)\n",
    "chl_valid_pct = np.sum(valid_chl) / processed_chl.size * 100\n",
    "print(f\"\\nCHL:\")\n",
    "print(f\"  Valid data: {chl_valid_pct:.1f}%\")\n",
    "if np.any(valid_chl):\n",
    "    print(f\"  Range: {np.nanmin(processed_chl):.4f} to {np.nanmax(processed_chl):.4f} mg/mÂ³\")\n",
    "    print(f\"  Mean: {np.nanmean(processed_chl):.4f} mg/mÂ³\")\n",
    "\n",
    "# SST\n",
    "valid_sst = ~np.isnan(processed_sst)\n",
    "sst_valid_pct = np.sum(valid_sst) / processed_sst.size * 100\n",
    "print(f\"\\nSST:\")\n",
    "print(f\"  Valid data: {sst_valid_pct:.1f}%\")\n",
    "if np.any(valid_sst):\n",
    "    print(f\"  Range: {np.nanmin(processed_sst):.2f} to {np.nanmax(processed_sst):.2f} Â°C\")\n",
    "    print(f\"  Mean: {np.nanmean(processed_sst):.2f} Â°C\")\n",
    "\n",
    "# Salinity\n",
    "valid_so = ~np.isnan(processed_so)\n",
    "so_valid_pct = np.sum(valid_so) / processed_so.size * 100\n",
    "print(f\"\\nSalinity:\")\n",
    "print(f\"  Valid data: {so_valid_pct:.1f}%\")\n",
    "if np.any(valid_so):\n",
    "    print(f\"  Range: {np.nanmin(processed_so):.2f} to {np.nanmax(processed_so):.2f} PSU\")\n",
    "    print(f\"  Mean: {np.nanmean(processed_so):.2f} PSU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/processed/processed_data.npz\n",
      "\n",
      "Data shape: (1461, 28, 29)\n",
      "Grid size: 28 x 29\n",
      "Time steps: 1461\n"
     ]
    }
   ],
   "source": [
    "# Save processed data as numpy arrays\n",
    "# Format: [time, lat, lon]\n",
    "\n",
    "np.savez_compressed(\n",
    "    f\"{OUTPUT_DIR}/processed_data.npz\",\n",
    "    chl=processed_chl,\n",
    "    sst=processed_sst,\n",
    "    salinity=processed_so,\n",
    "    lat_grid=lat_grid,\n",
    "    lon_grid=lon_grid,\n",
    "    time_indices=np.arange(n_times)\n",
    ")\n",
    "\n",
    "print(f\"Processed data saved to {OUTPUT_DIR}/processed_data.npz\")\n",
    "print(f\"\\nData shape: {processed_chl.shape}\")\n",
    "print(f\"Grid size: {len(lat_grid)} x {len(lon_grid)}\")\n",
    "print(f\"Time steps: {n_times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESSING SUMMARY ===\n",
      "\n",
      "âœ… Data preprocessing completed!\n",
      "\n",
      "What was done:\n",
      "1. âœ… SST converted: Kelvin â†’ Celcius\n",
      "2. âœ… Surface salinity extracted (depth=0)\n",
      "3. âœ… All data resampled to uniform grid ({TARGET_RESOLUTION}Â°)\n",
      "4. âœ… Data cropped to bounding box\n",
      "5. âœ… Missing values handled (NaN)\n",
      "\n",
      "Next Steps:\n",
      "- Fase 3: HSI Calculation\n",
      "  - Calculate HSI_CHL, HSI_SST, HSI_SO\n",
      "  - Calculate HSI_total = (HSI_CHL Ã— HSI_SST Ã— HSI_SO)^(1/3)\n",
      "\n",
      "Note: This was a sample run ({n_times} time steps).\n",
      "To process all data, set SAMPLE_SIZE = None in cell 8.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PREPROCESSING SUMMARY ===\")\n",
    "print(\"\\nâœ… Data preprocessing completed!\")\n",
    "print(\"\\nWhat was done:\")\n",
    "print(\"1. âœ… SST converted: Kelvin â†’ Celcius\")\n",
    "print(\"2. âœ… Surface salinity extracted (depth=0)\")\n",
    "print(\"3. âœ… All data resampled to uniform grid ({TARGET_RESOLUTION}Â°)\")\n",
    "print(\"4. âœ… Data cropped to bounding box\")\n",
    "print(\"5. âœ… Missing values handled (NaN)\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Fase 3: HSI Calculation\")\n",
    "print(\"  - Calculate HSI_CHL, HSI_SST, HSI_SO\")\n",
    "print(\"  - Calculate HSI_total = (HSI_CHL Ã— HSI_SST Ã— HSI_SO)^(1/3)\")\n",
    "print(\"\\nNote: This was a sample run ({n_times} time steps).\")\n",
    "print(\"To process all data, set SAMPLE_SIZE = None in cell 8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
