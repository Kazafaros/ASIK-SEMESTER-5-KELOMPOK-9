{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 9: Monthly HSI Prediction Model dengan Parameter Oceanografi\n",
    "\n",
    "Model prediksi HSI per bulan menggunakan:\n",
    "- **ARIMA** untuk time series forecasting\n",
    "- **Parameter Oceanografi**: SST, SO, CHL\n",
    "- **Data Training**: 2021-2024 (48 bulan)\n",
    "- **Prediksi**: 12 bulan ke depan (2025)\n",
    "\n",
    "## Workflow:\n",
    "1. Load data oceanografi harian (NetCDF)\n",
    "2. Aggregate ke bulanan\n",
    "3. Align ke grid yang sama\n",
    "4. Build regression model: HSI = f(SST, SO, CHL)\n",
    "5. Train ARIMA per grid point\n",
    "6. Predict 12 bulan ke depan\n",
    "7. Export GeoJSON untuk setiap bulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ netCDF4 imported\n",
      "‚úÖ pmdarima imported\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NetCDF\n",
    "try:\n",
    "    import netCDF4\n",
    "    print(\"‚úÖ netCDF4 imported\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'netCDF4'])\n",
    "    import netCDF4\n",
    "    print(\"‚úÖ netCDF4 installed\")\n",
    "\n",
    "# ARIMA\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    print(\"‚úÖ pmdarima imported\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'pmdarima'])\n",
    "    from pmdarima import auto_arima\n",
    "    print(\"‚úÖ pmdarima installed\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Oceanographic Data (NetCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Oceanographic Data ===\n",
      "\n",
      "Loading SST...\n",
      "  SST shape: (1461, 27, 29)\n",
      "  SST range: -6.78 to -5.47 (lat), 104.58 to 105.98 (lon)\n",
      "\n",
      "Loading SO (Salinity)...\n",
      "  SO shape: (1461, 50, 16, 17)\n",
      "  SO depth levels: 50\n",
      "  SO surface shape: (1461, 16, 17)\n",
      "\n",
      "Loading CHL (Chlorophyll-a)...\n",
      "  CHL shape: (1461, 32, 34)\n",
      "  CHL range: -6.77 to -5.48 (lat), 104.56 to 105.94 (lon)\n",
      "\n",
      "‚úÖ All oceanographic data loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading Oceanographic Data ===\")\n",
    "\n",
    "# File paths\n",
    "sst_file = '../SST 21-24.nc'\n",
    "so_file = '../SO 21-24.nc'\n",
    "chl_file = '../CHL 21-24.nc'\n",
    "\n",
    "# Load SST\n",
    "print(\"\\nLoading SST...\")\n",
    "ds_sst = netCDF4.Dataset(sst_file)\n",
    "sst_data = ds_sst.variables['analysed_sst'][:]\n",
    "lat_sst = ds_sst.variables['latitude'][:]\n",
    "lon_sst = ds_sst.variables['longitude'][:]\n",
    "time_sst = ds_sst.variables['time'][:]\n",
    "print(f\"  SST shape: {sst_data.shape}\")\n",
    "print(f\"  SST range: {lat_sst.min():.2f} to {lat_sst.max():.2f} (lat), {lon_sst.min():.2f} to {lon_sst.max():.2f} (lon)\")\n",
    "\n",
    "# Load SO (Salinity)\n",
    "print(\"\\nLoading SO (Salinity)...\")\n",
    "ds_so = netCDF4.Dataset(so_file)\n",
    "so_data = ds_so.variables['so'][:]\n",
    "lat_so = ds_so.variables['latitude'][:]\n",
    "lon_so = ds_so.variables['longitude'][:]\n",
    "depth_so = ds_so.variables['depth'][:]\n",
    "print(f\"  SO shape: {so_data.shape}\")\n",
    "print(f\"  SO depth levels: {len(depth_so)}\")\n",
    "# Use surface layer (depth index 0)\n",
    "so_data = so_data[:, 0, :, :]\n",
    "print(f\"  SO surface shape: {so_data.shape}\")\n",
    "\n",
    "# Load CHL (Chlorophyll-a)\n",
    "print(\"\\nLoading CHL (Chlorophyll-a)...\")\n",
    "ds_chl = netCDF4.Dataset(chl_file)\n",
    "chl_data = ds_chl.variables['CHL'][:]\n",
    "lat_chl = ds_chl.variables['latitude'][:]\n",
    "lon_chl = ds_chl.variables['longitude'][:]\n",
    "print(f\"  CHL shape: {chl_data.shape}\")\n",
    "print(f\"  CHL range: {lat_chl.min():.2f} to {lat_chl.max():.2f} (lat), {lon_chl.min():.2f} to {lon_chl.max():.2f} (lon)\")\n",
    "\n",
    "print(\"\\n‚úÖ All oceanographic data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Time and Aggregate to Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Converting Time and Aggregating to Monthly ===\n",
      "\n",
      "Date range: 2021-01-01 to 2024-12-31\n",
      "Total days: 1461\n",
      "\n",
      "Unique months: 48\n",
      "First month: 2021-01\n",
      "Last month: 2024-12\n",
      "\n",
      "‚úÖ Time conversion complete!\n"
     ]
    }
   ],
   "source": [
    "# Convert time from seconds since 1970-01-01\n",
    "print(\"=== Converting Time and Aggregating to Monthly ===\")\n",
    "\n",
    "# Create datetime array\n",
    "base_date = datetime(1970, 1, 1)\n",
    "dates = [base_date + timedelta(seconds=int(t)) for t in time_sst]\n",
    "\n",
    "print(f\"\\nDate range: {dates[0].date()} to {dates[-1].date()}\")\n",
    "print(f\"Total days: {len(dates)}\")\n",
    "\n",
    "# Create DataFrame for time tracking\n",
    "df_time = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'year': [d.year for d in dates],\n",
    "    'month': [d.month for d in dates],\n",
    "    'day': [d.day for d in dates]\n",
    "})\n",
    "\n",
    "# Get unique year-month combinations\n",
    "df_time['year_month'] = df_time['year'].astype(str) + '-' + df_time['month'].astype(str).str.zfill(2)\n",
    "unique_months = df_time['year_month'].unique()\n",
    "\n",
    "print(f\"\\nUnique months: {len(unique_months)}\")\n",
    "print(f\"First month: {unique_months[0]}\")\n",
    "print(f\"Last month: {unique_months[-1]}\")\n",
    "\n",
    "# Create mapping of month to indices\n",
    "month_indices = {}\n",
    "for month in unique_months:\n",
    "    indices = df_time[df_time['year_month'] == month].index.tolist()\n",
    "    month_indices[month] = indices\n",
    "\n",
    "print(f\"\\n‚úÖ Time conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate Daily Data to Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregating to Monthly Means ===\n",
      "\n",
      "Reference grid (CHL): 32 x 34\n",
      "Number of months: 48\n",
      "\n",
      "Aggregating SST...\n",
      "  Processed 12/48 months\n",
      "  Processed 24/48 months\n",
      "  Processed 36/48 months\n",
      "  Processed 48/48 months\n",
      "‚úÖ SST aggregated\n",
      "\n",
      "Aggregating SO...\n",
      "  Processed 12/48 months\n",
      "  Processed 24/48 months\n",
      "  Processed 36/48 months\n",
      "  Processed 48/48 months\n",
      "‚úÖ SO aggregated\n",
      "\n",
      "Aggregating CHL...\n",
      "  Processed 12/48 months\n",
      "  Processed 24/48 months\n",
      "  Processed 36/48 months\n",
      "  Processed 48/48 months\n",
      "‚úÖ CHL aggregated\n",
      "\n",
      "‚úÖ All data aggregated to monthly!\n",
      "  SST monthly shape: (48, 32, 34)\n",
      "  SO monthly shape: (48, 32, 34)\n",
      "  CHL monthly shape: (48, 32, 34)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate to monthly means\n",
    "print(\"=== Aggregating to Monthly Means ===\")\n",
    "\n",
    "# Use CHL grid as reference (finest resolution)\n",
    "n_lat_ref = len(lat_chl)\n",
    "n_lon_ref = len(lon_chl)\n",
    "n_months = len(unique_months)\n",
    "\n",
    "print(f\"\\nReference grid (CHL): {n_lat_ref} x {n_lon_ref}\")\n",
    "print(f\"Number of months: {n_months}\")\n",
    "\n",
    "# Initialize monthly arrays\n",
    "sst_monthly = np.full((n_months, n_lat_ref, n_lon_ref), np.nan)\n",
    "so_monthly = np.full((n_months, n_lat_ref, n_lon_ref), np.nan)\n",
    "chl_monthly = np.full((n_months, n_lat_ref, n_lon_ref), np.nan)\n",
    "\n",
    "# Aggregate SST\n",
    "print(\"\\nAggregating SST...\")\n",
    "lon_mesh_sst, lat_mesh_sst = np.meshgrid(lon_sst, lat_sst)\n",
    "lon_mesh_ref, lat_mesh_ref = np.meshgrid(lon_chl, lat_chl)\n",
    "\n",
    "for m_idx, month in enumerate(unique_months):\n",
    "    indices = month_indices[month]\n",
    "    \n",
    "    # Calculate monthly mean for SST\n",
    "    sst_month_data = np.nanmean(sst_data[indices, :, :], axis=0)\n",
    "    \n",
    "    # Interpolate to reference grid\n",
    "    points_sst = np.column_stack((lon_mesh_sst.ravel(), lat_mesh_sst.ravel()))\n",
    "    values_sst = sst_month_data.ravel()\n",
    "    points_ref = np.column_stack((lon_mesh_ref.ravel(), lat_mesh_ref.ravel()))\n",
    "    \n",
    "    sst_monthly[m_idx, :, :] = griddata(points_sst, values_sst, points_ref, method='linear', fill_value=np.nan).reshape(lon_mesh_ref.shape)\n",
    "    \n",
    "    if (m_idx + 1) % 12 == 0:\n",
    "        print(f\"  Processed {m_idx + 1}/{n_months} months\")\n",
    "\n",
    "print(\"‚úÖ SST aggregated\")\n",
    "\n",
    "# Aggregate SO\n",
    "print(\"\\nAggregating SO...\")\n",
    "lon_mesh_so, lat_mesh_so = np.meshgrid(lon_so, lat_so)\n",
    "\n",
    "for m_idx, month in enumerate(unique_months):\n",
    "    indices = month_indices[month]\n",
    "    \n",
    "    # Calculate monthly mean for SO\n",
    "    so_month_data = np.nanmean(so_data[indices, :, :], axis=0)\n",
    "    \n",
    "    # Interpolate to reference grid\n",
    "    points_so = np.column_stack((lon_mesh_so.ravel(), lat_mesh_so.ravel()))\n",
    "    values_so = so_month_data.ravel()\n",
    "    \n",
    "    so_monthly[m_idx, :, :] = griddata(points_so, values_so, points_ref, method='linear', fill_value=np.nan).reshape(lon_mesh_ref.shape)\n",
    "    \n",
    "    if (m_idx + 1) % 12 == 0:\n",
    "        print(f\"  Processed {m_idx + 1}/{n_months} months\")\n",
    "\n",
    "print(\"‚úÖ SO aggregated\")\n",
    "\n",
    "# CHL is already at reference grid\n",
    "print(\"\\nAggregating CHL...\")\n",
    "for m_idx, month in enumerate(unique_months):\n",
    "    indices = month_indices[month]\n",
    "    chl_monthly[m_idx, :, :] = np.nanmean(chl_data[indices, :, :], axis=0)\n",
    "    \n",
    "    if (m_idx + 1) % 12 == 0:\n",
    "        print(f\"  Processed {m_idx + 1}/{n_months} months\")\n",
    "\n",
    "print(\"‚úÖ CHL aggregated\")\n",
    "\n",
    "print(f\"\\n‚úÖ All data aggregated to monthly!\")\n",
    "print(f\"  SST monthly shape: {sst_monthly.shape}\")\n",
    "print(f\"  SO monthly shape: {so_monthly.shape}\")\n",
    "print(f\"  CHL monthly shape: {chl_monthly.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Historical HSI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Historical HSI Data ===\n",
      "‚úÖ HSI data loaded!\n",
      "  Shape: (48, 28, 29)\n",
      "  Grid: 28 x 29\n",
      "  Date range: 2021-01 to 2024-12\n",
      "\n",
      "Interpolating HSI to reference grid...\n",
      "  Interpolated HSI shape: (48, 32, 34)\n",
      "\n",
      "‚úÖ HSI data ready!\n"
     ]
    }
   ],
   "source": [
    "# Load monthly HSI data\n",
    "print(\"=== Loading Historical HSI Data ===\")\n",
    "\n",
    "MONTHLY_DATA_FILE = '../data/processed/monthly_hsi_data.npz'\n",
    "\n",
    "if not os.path.exists(MONTHLY_DATA_FILE):\n",
    "    raise FileNotFoundError(f\"Monthly HSI data not found! Please run monthly aggregation notebook first.\")\n",
    "\n",
    "data = np.load(MONTHLY_DATA_FILE)\n",
    "\n",
    "monthly_hsi = data['hsi_total']  # Shape: [48 months, lat, lon]\n",
    "lat_grid_hsi = data['lat_grid']\n",
    "lon_grid_hsi = data['lon_grid']\n",
    "months_hsi = data['months']\n",
    "\n",
    "if isinstance(months_hsi, np.ndarray):\n",
    "    months_hsi = months_hsi.tolist()\n",
    "\n",
    "print(f\"‚úÖ HSI data loaded!\")\n",
    "print(f\"  Shape: {monthly_hsi.shape}\")\n",
    "print(f\"  Grid: {len(lat_grid_hsi)} x {len(lon_grid_hsi)}\")\n",
    "print(f\"  Date range: {months_hsi[0]} to {months_hsi[-1]}\")\n",
    "\n",
    "# Interpolate HSI to CHL grid if needed\n",
    "if len(lat_grid_hsi) != n_lat_ref or len(lon_grid_hsi) != n_lon_ref:\n",
    "    print(\"\\nInterpolating HSI to reference grid...\")\n",
    "    lon_mesh_hsi, lat_mesh_hsi = np.meshgrid(lon_grid_hsi, lat_grid_hsi)\n",
    "    \n",
    "    hsi_monthly_interp = np.full((monthly_hsi.shape[0], n_lat_ref, n_lon_ref), np.nan)\n",
    "    \n",
    "    for t in range(monthly_hsi.shape[0]):\n",
    "        points_hsi = np.column_stack((lon_mesh_hsi.ravel(), lat_mesh_hsi.ravel()))\n",
    "        values_hsi = monthly_hsi[t, :, :].ravel()\n",
    "        \n",
    "        hsi_monthly_interp[t, :, :] = griddata(points_hsi, values_hsi, points_ref, method='linear', fill_value=np.nan).reshape(lon_mesh_ref.shape)\n",
    "    \n",
    "    monthly_hsi = hsi_monthly_interp\n",
    "    print(f\"  Interpolated HSI shape: {monthly_hsi.shape}\")\n",
    "else:\n",
    "    print(\"\\nHSI already on reference grid\")\n",
    "\n",
    "print(f\"\\n‚úÖ HSI data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Regression Model: HSI = f(SST, SO, CHL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Regression Model ===\n",
      "\n",
      "Total samples: 52,224\n",
      "Valid samples: 32,122\n",
      "Removed: 20,102 samples\n",
      "\n",
      "Training set: 25,697 samples\n",
      "Test set: 6,425 samples\n",
      "\n",
      "Training Linear Regression...\n",
      "  R¬≤: 0.8508, MAE: 0.0344\n",
      "\n",
      "Training Random Forest...\n",
      "  R¬≤: 0.8780, MAE: 0.0237\n",
      "\n",
      "‚úÖ Best model: RANDOM_FOREST (R¬≤ = 0.8780)\n",
      "\n",
      "Feature Importance:\n",
      "  SST: 0.1913 (19.1%)\n",
      "  SO:  0.7262 (72.6%)\n",
      "  CHL: 0.0826 (8.3%)\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "print(\"=== Building Regression Model ===\")\n",
    "\n",
    "# Flatten spatial dimensions\n",
    "X_sst = sst_monthly.reshape(n_months, -1)  # [months, grid_points]\n",
    "X_so = so_monthly.reshape(n_months, -1)\n",
    "X_chl = chl_monthly.reshape(n_months, -1)\n",
    "y_hsi = monthly_hsi.reshape(n_months, -1)\n",
    "\n",
    "# Stack features: [months*grid_points, 3]\n",
    "X = np.column_stack((X_sst.ravel(), X_so.ravel(), X_chl.ravel()))\n",
    "y = y_hsi.ravel()\n",
    "\n",
    "# Remove NaN values\n",
    "valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "X_clean = X[valid_mask]\n",
    "y_clean = y[valid_mask]\n",
    "\n",
    "print(f\"\\nTotal samples: {len(X):,}\")\n",
    "print(f\"Valid samples: {len(X_clean):,}\")\n",
    "print(f\"Removed: {len(X) - len(X_clean):,} samples\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "split_idx = int(0.8 * len(X_scaled))\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y_clean[:split_idx], y_clean[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Train Linear Regression\n",
    "print(\"\\nTraining Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "print(f\"  R¬≤: {lr_r2:.4f}, MAE: {lr_mae:.4f}\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "print(f\"  R¬≤: {rf_r2:.4f}, MAE: {rf_mae:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "if rf_r2 > lr_r2:\n",
    "    best_model = rf_model\n",
    "    best_model_name = 'random_forest'\n",
    "    best_r2 = rf_r2\n",
    "else:\n",
    "    best_model = lr_model\n",
    "    best_model_name = 'linear_regression'\n",
    "    best_r2 = lr_r2\n",
    "\n",
    "print(f\"\\n‚úÖ Best model: {best_model_name.upper()} (R¬≤ = {best_r2:.4f})\")\n",
    "\n",
    "# Feature importance\n",
    "if best_model_name == 'random_forest':\n",
    "    importance = rf_model.feature_importances_\n",
    "    print(f\"\\nFeature Importance:\")\n",
    "    print(f\"  SST: {importance[0]:.4f} ({importance[0]*100:.1f}%)\")\n",
    "    print(f\"  SO:  {importance[1]:.4f} ({importance[1]*100:.1f}%)\")\n",
    "    print(f\"  CHL: {importance[2]:.4f} ({importance[2]*100:.1f}%)\")\n",
    "else:\n",
    "    coef = lr_model.coef_\n",
    "    print(f\"\\nLinear Coefficients:\")\n",
    "    print(f\"  SST: {coef[0]:.6f}\")\n",
    "    print(f\"  SO:  {coef[1]:.6f}\")\n",
    "    print(f\"  CHL: {coef[2]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train ARIMA Models per Grid Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ARIMA Models ===\n",
      "Training 1088 models...\n",
      "  Trained 8/32 rows...\n",
      "  Trained 16/32 rows...\n",
      "  Trained 24/32 rows...\n",
      "  Trained 32/32 rows...\n",
      "\n",
      "‚úÖ ARIMA training complete!\n",
      "  Successfully trained: 1,030 models\n",
      "  Failed: 58 points\n"
     ]
    }
   ],
   "source": [
    "# Train ARIMA for each grid point\n",
    "print(\"=== Training ARIMA Models ===\")\n",
    "print(f\"Training {n_lat_ref * n_lon_ref} models...\")\n",
    "\n",
    "trained_models = {}\n",
    "failed_points = 0\n",
    "\n",
    "for i in range(n_lat_ref):\n",
    "    for j in range(n_lon_ref):\n",
    "        time_series = monthly_hsi[:, i, j]\n",
    "        \n",
    "        # Skip if invalid\n",
    "        if np.all(np.isnan(time_series)) or np.nanstd(time_series) == 0:\n",
    "            failed_points += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Train ARIMA\n",
    "            model = auto_arima(\n",
    "                time_series,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=2, max_q=2,\n",
    "                seasonal=False,\n",
    "                stepwise=True,\n",
    "                suppress_warnings=True,\n",
    "                error_action='ignore',\n",
    "                max_order=4,\n",
    "                n_jobs=1\n",
    "            )\n",
    "            \n",
    "            trained_models[(i, j)] = model\n",
    "        except Exception as e:\n",
    "            failed_points += 1\n",
    "            continue\n",
    "    \n",
    "    if (i + 1) % 8 == 0:\n",
    "        print(f\"  Trained {i+1}/{n_lat_ref} rows...\")\n",
    "\n",
    "print(f\"\\n‚úÖ ARIMA training complete!\")\n",
    "print(f\"  Successfully trained: {len(trained_models):,} models\")\n",
    "print(f\"  Failed: {failed_points:,} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predict for Next 12 Months (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Predicting for 2025 ===\n",
      "Predicting month 1/12...\n",
      "  ‚úÖ Month 1: 1,030 valid predictions\n",
      "Predicting month 2/12...\n",
      "  ‚úÖ Month 2: 1,030 valid predictions\n",
      "Predicting month 3/12...\n",
      "  ‚úÖ Month 3: 1,030 valid predictions\n",
      "Predicting month 4/12...\n",
      "  ‚úÖ Month 4: 1,030 valid predictions\n",
      "Predicting month 5/12...\n",
      "  ‚úÖ Month 5: 1,030 valid predictions\n",
      "Predicting month 6/12...\n",
      "  ‚úÖ Month 6: 1,030 valid predictions\n",
      "Predicting month 7/12...\n",
      "  ‚úÖ Month 7: 1,030 valid predictions\n",
      "Predicting month 8/12...\n",
      "  ‚úÖ Month 8: 1,030 valid predictions\n",
      "Predicting month 9/12...\n",
      "  ‚úÖ Month 9: 1,030 valid predictions\n",
      "Predicting month 10/12...\n",
      "  ‚úÖ Month 10: 1,030 valid predictions\n",
      "Predicting month 11/12...\n",
      "  ‚úÖ Month 11: 1,030 valid predictions\n",
      "Predicting month 12/12...\n",
      "  ‚úÖ Month 12: 1,030 valid predictions\n",
      "\n",
      "‚úÖ Prediction complete for all 12 months!\n"
     ]
    }
   ],
   "source": [
    "# Predict for 12 months ahead\n",
    "print(\"=== Predicting for 2025 ===\")\n",
    "\n",
    "PREDICTION_MONTHS = 12\n",
    "PREDICTION_YEAR = 2025\n",
    "\n",
    "# Store predictions: {month: [n_lat, n_lon] array}\n",
    "predictions_2025 = {}\n",
    "\n",
    "for month in range(1, PREDICTION_MONTHS + 1):\n",
    "    print(f\"Predicting month {month}/12...\")\n",
    "    \n",
    "    prediction_array = np.full((n_lat_ref, n_lon_ref), np.nan)\n",
    "    \n",
    "    for (i, j), model in trained_models.items():\n",
    "        try:\n",
    "            # Predict 1 month ahead\n",
    "            forecast = model.predict(n_periods=month)\n",
    "            \n",
    "            # Get the last prediction (for this month)\n",
    "            hsi_pred = forecast[-1]\n",
    "            \n",
    "            # Clip to valid range [0, 1]\n",
    "            hsi_pred = np.clip(hsi_pred, 0.0, 1.0)\n",
    "            \n",
    "            prediction_array[i, j] = hsi_pred\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    predictions_2025[month] = prediction_array\n",
    "    valid_count = np.sum(~np.isnan(prediction_array))\n",
    "    print(f\"  ‚úÖ Month {month}: {valid_count:,} valid predictions\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction complete for all 12 months!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create GeoJSON for Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GeoJSON creation function defined!\n"
     ]
    }
   ],
   "source": [
    "def create_monthly_geojson(month, year, prediction_array, lat_grid, lon_grid):\n",
    "    \"\"\"\n",
    "    Create GeoJSON from monthly prediction\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
    "    \n",
    "    for i in range(len(lat_grid)):\n",
    "        for j in range(len(lon_grid)):\n",
    "            lat = lat_mesh[i, j]\n",
    "            lon = lon_mesh[i, j]\n",
    "            hsi_val = float(prediction_array[i, j]) if not np.isnan(prediction_array[i, j]) else None\n",
    "            \n",
    "            if hsi_val is None:\n",
    "                continue\n",
    "            \n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [float(lon), float(lat)]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"hsi\": hsi_val,\n",
    "                    \"year\": year,\n",
    "                    \"month\": month,\n",
    "                    \"date\": f\"{year}-{str(month).zfill(2)}-01\",\n",
    "                    \"is_prediction\": True\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            features.append(feature)\n",
    "    \n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"properties\": {\n",
    "            \"year\": year,\n",
    "            \"month\": month,\n",
    "            \"date\": f\"{year}-{str(month).zfill(2)}\",\n",
    "            \"model_type\": \"arima_with_oceanography\",\n",
    "            \"features_count\": len(features)\n",
    "        },\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    return geojson\n",
    "\n",
    "print(\"‚úÖ GeoJSON creation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exporting Predictions ===\n",
      "Output directory: ../data/predictions/monthly_2025\n",
      "  ‚úÖ hsi_prediction_2025_01.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_02.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_03.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_04.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_05.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_06.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_07.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_08.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_09.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_10.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_11.geojson (1,030 features)\n",
      "  ‚úÖ hsi_prediction_2025_12.geojson (1,030 features)\n",
      "\n",
      "‚úÖ Exported 12 monthly prediction files!\n"
     ]
    }
   ],
   "source": [
    "# Export all monthly predictions\n",
    "OUTPUT_DIR = '../data/predictions/monthly_2025'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"=== Exporting Predictions ===\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "exported_files = []\n",
    "\n",
    "for month in range(1, PREDICTION_MONTHS + 1):\n",
    "    prediction_array = predictions_2025[month]\n",
    "    geojson = create_monthly_geojson(month, PREDICTION_YEAR, prediction_array, lat_chl, lon_chl)\n",
    "    \n",
    "    filename = f\"hsi_prediction_{PREDICTION_YEAR}_{str(month).zfill(2)}.geojson\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    exported_files.append({\n",
    "        'month': month,\n",
    "        'year': PREDICTION_YEAR,\n",
    "        'file': filename,\n",
    "        'features': len(geojson['features'])\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úÖ {filename} ({len(geojson['features']):,} features)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Exported {len(exported_files)} monthly prediction files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calculating Statistics ===\n",
      "Month  1: Mean=0.7801, Min=0.6999, Max=0.8462, Std=0.0290\n",
      "Month  2: Mean=0.7444, Min=0.6532, Max=0.8129, Std=0.0253\n",
      "Month  3: Mean=0.7130, Min=0.6209, Max=0.8043, Std=0.0284\n",
      "Month  4: Mean=0.6974, Min=0.6051, Max=0.8020, Std=0.0333\n",
      "Month  5: Mean=0.6886, Min=0.5947, Max=0.8006, Std=0.0377\n",
      "Month  6: Mean=0.6852, Min=0.5714, Max=0.7998, Std=0.0394\n",
      "Month  7: Mean=0.6859, Min=0.5640, Max=0.7993, Std=0.0387\n",
      "Month  8: Mean=0.6889, Min=0.5703, Max=0.7991, Std=0.0362\n",
      "Month  9: Mean=0.6931, Min=0.5868, Max=0.7989, Std=0.0328\n",
      "Month 10: Mean=0.6971, Min=0.6092, Max=0.7988, Std=0.0295\n",
      "Month 11: Mean=0.7005, Min=0.6274, Max=0.7988, Std=0.0270\n",
      "Month 12: Mean=0.7026, Min=0.6274, Max=0.7987, Std=0.0256\n",
      "\n",
      "‚úÖ Statistics calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for each month\n",
    "print(\"=== Calculating Statistics ===\")\n",
    "\n",
    "monthly_stats = {}\n",
    "\n",
    "for month in range(1, PREDICTION_MONTHS + 1):\n",
    "    prediction_array = predictions_2025[month]\n",
    "    hsi_values = prediction_array[~np.isnan(prediction_array)]\n",
    "    \n",
    "    if len(hsi_values) == 0:\n",
    "        continue\n",
    "    \n",
    "    sorted_vals = np.sort(hsi_values)\n",
    "    \n",
    "    stats = {\n",
    "        'month': month,\n",
    "        'year': PREDICTION_YEAR,\n",
    "        'count': int(len(hsi_values)),\n",
    "        'min': float(sorted_vals[0]),\n",
    "        'max': float(sorted_vals[-1]),\n",
    "        'mean': float(np.mean(hsi_values)),\n",
    "        'median': float(np.median(hsi_values)),\n",
    "        'std': float(np.std(hsi_values)),\n",
    "        'q25': float(sorted_vals[int(len(sorted_vals) * 0.25)]),\n",
    "        'q75': float(sorted_vals[int(len(sorted_vals) * 0.75)])\n",
    "    }\n",
    "    \n",
    "    monthly_stats[month] = stats\n",
    "    \n",
    "    print(f\"Month {month:2d}: Mean={stats['mean']:.4f}, Min={stats['min']:.4f}, Max={stats['max']:.4f}, Std={stats['std']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Statistics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata saved to ../data/predictions/monthly_2025\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive metadata\n",
    "metadata = {\n",
    "    \"title\": \"HSI Monthly Prediction 2025\",\n",
    "    \"description\": \"Predicted Habitat Suitability Index for 12 months of 2025 using ARIMA with oceanographic parameters\",\n",
    "    \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_type\": \"arima_with_oceanography\",\n",
    "    \"prediction_year\": PREDICTION_YEAR,\n",
    "    \"prediction_months\": PREDICTION_MONTHS,\n",
    "    \"training_data\": {\n",
    "        \"start\": months_hsi[0],\n",
    "        \"end\": months_hsi[-1],\n",
    "        \"total_months\": len(months_hsi)\n",
    "    },\n",
    "    \"oceanographic_parameters\": {\n",
    "        \"sst\": {\n",
    "            \"name\": \"Sea Surface Temperature\",\n",
    "            \"unit\": \"Kelvin\",\n",
    "            \"source\": \"OSTIA L4\"\n",
    "        },\n",
    "        \"so\": {\n",
    "            \"name\": \"Salinity\",\n",
    "            \"unit\": \"1e-3 (PSU)\",\n",
    "            \"source\": \"GLORYS12V1\",\n",
    "            \"depth\": \"Surface layer\"\n",
    "        },\n",
    "        \"chl\": {\n",
    "            \"name\": \"Chlorophyll-a\",\n",
    "            \"unit\": \"mg/m¬≥\",\n",
    "            \"source\": \"GlobColour\"\n",
    "        }\n",
    "    },\n",
    "    \"regression_model\": {\n",
    "        \"type\": best_model_name,\n",
    "        \"r2_score\": float(best_r2),\n",
    "        \"mae\": float(lr_mae if best_model_name == 'linear_regression' else rf_mae)\n",
    "    },\n",
    "    \"models_trained\": len(trained_models),\n",
    "    \"grid_info\": {\n",
    "        \"lat_count\": n_lat_ref,\n",
    "        \"lon_count\": n_lon_ref,\n",
    "        \"total_points\": n_lat_ref * n_lon_ref,\n",
    "        \"lat_range\": [float(lat_chl.min()), float(lat_chl.max())],\n",
    "        \"lon_range\": [float(lon_chl.min()), float(lon_chl.max())]\n",
    "    },\n",
    "    \"monthly_statistics\": monthly_stats,\n",
    "    \"available_predictions\": exported_files,\n",
    "    \"total_predictions\": len(exported_files),\n",
    "    \"note\": \"Predictions are based on ARIMA models trained on historical data (2021-2024) with oceanographic parameters. Use for planning and analysis purposes.\"\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = os.path.join(OUTPUT_DIR, 'metadata.json')\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Metadata saved to {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MONTHLY HSI PREDICTION MODEL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Monthly HSI prediction model completed successfully!\n",
      "\n",
      "üìä What was done:\n",
      "  1. ‚úÖ Loaded oceanographic data (SST, SO, CHL) from NetCDF\n",
      "  2. ‚úÖ Aggregated daily data to monthly means\n",
      "  3. ‚úÖ Aligned all data to common grid\n",
      "  4. ‚úÖ Built regression model: HSI = f(SST, SO, CHL)\n",
      "  5. ‚úÖ Trained ARIMA models for 1,030 grid points\n",
      "  6. ‚úÖ Predicted HSI for 12 months of 2025\n",
      "  7. ‚úÖ Generated GeoJSON files for each month\n",
      "  8. ‚úÖ Calculated monthly statistics\n",
      "  9. ‚úÖ Created comprehensive metadata\n",
      "\n",
      "üìÅ Output files:\n",
      "  - 12 monthly GeoJSON files in ../data/predictions/monthly_2025\n",
      "  - metadata.json with all model information\n",
      "\n",
      "üìà Model Performance:\n",
      "  - Best Model: RANDOM_FOREST\n",
      "  - R¬≤ Score: 0.8780\n",
      "  - Training Data: 48 months (2021-2024)\n",
      "  - Grid Points: 1,030 / 1,088\n",
      "\n",
      "üöÄ Next Steps:\n",
      "  1. Backend: Create API endpoints for predictions\n",
      "  2. Frontend: Integrate visualization in analysis.html\n",
      "  3. Testing: Validate predictions with domain experts\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Jupyter monthly prediction phase complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MONTHLY HSI PREDICTION MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Monthly HSI prediction model completed successfully!\")\n",
    "\n",
    "print(\"\\nüìä What was done:\")\n",
    "print(\"  1. ‚úÖ Loaded oceanographic data (SST, SO, CHL) from NetCDF\")\n",
    "print(\"  2. ‚úÖ Aggregated daily data to monthly means\")\n",
    "print(\"  3. ‚úÖ Aligned all data to common grid\")\n",
    "print(\"  4. ‚úÖ Built regression model: HSI = f(SST, SO, CHL)\")\n",
    "print(f\"  5. ‚úÖ Trained ARIMA models for {len(trained_models):,} grid points\")\n",
    "print(f\"  6. ‚úÖ Predicted HSI for 12 months of {PREDICTION_YEAR}\")\n",
    "print(\"  7. ‚úÖ Generated GeoJSON files for each month\")\n",
    "print(\"  8. ‚úÖ Calculated monthly statistics\")\n",
    "print(\"  9. ‚úÖ Created comprehensive metadata\")\n",
    "\n",
    "print(\"\\nüìÅ Output files:\")\n",
    "print(f\"  - 12 monthly GeoJSON files in {OUTPUT_DIR}\")\n",
    "print(f\"  - metadata.json with all model information\")\n",
    "\n",
    "print(\"\\nüìà Model Performance:\")\n",
    "print(f\"  - Best Model: {best_model_name.upper()}\")\n",
    "print(f\"  - R¬≤ Score: {best_r2:.4f}\")\n",
    "print(f\"  - Training Data: {len(months_hsi)} months (2021-2024)\")\n",
    "print(f\"  - Grid Points: {len(trained_models):,} / {n_lat_ref * n_lon_ref:,}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Backend: Create API endpoints for predictions\")\n",
    "print(\"  2. Frontend: Integrate visualization in analysis.html\")\n",
    "print(\"  3. Testing: Validate predictions with domain experts\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Jupyter monthly prediction phase complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
